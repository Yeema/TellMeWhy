{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request, session, g, redirect, url_for, \\\n",
    "     abort, render_template, flash, jsonify\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import spacy\n",
    "import operator\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from collections import Counter,defaultdict\n",
    "import geniatagger\n",
    "from geniatagger import GeniaTagger\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import spacy\n",
    "from nltk.tokenize import sent_tokenize,word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ab = re.compile(r\"\\w*'\\w*|\\w*’\\w*\")\n",
    "loss_del = re.compile(r'\\[ *- *([^\\[\\]]*?) *- *\\]')\n",
    "loss_add = re.compile(r'\\{\\ *\\+ *([^\\[\\]]*?) *\\+ *\\}')\n",
    "delandadd = re.compile(r'\\[- *([^\\[\\]]*?) *-\\] *\\{\\+ *([^\\[\\]]*?) *\\+\\}')\n",
    "deletion = re.compile(r'\\[- *([^\\[\\]]*?) *-\\]')\n",
    "addition = re.compile(r'\\{\\+ *([^\\[\\]]*?) *\\+\\}')\n",
    "braces = re.compile(r'\\[ *(.*?) *\\]')\n",
    "multi_delandadd = re.compile(r'(\\[-([^\\[\\]]*?)-\\] *\\{\\+([^\\{\\}]*?)\\+\\} *)+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger = GeniaTagger('/home/nlplab/yeema/geniataggerPython/geniatagger-3.0.2/geniatagger')\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictWord = defaultdict(lambda: defaultdict(list))\n",
    "phraseV = defaultdict(lambda: defaultdict(list))\n",
    "dictPhrase = defaultdict(lambda: defaultdict(list))\n",
    "dictDef = defaultdict(lambda: defaultdict(list))\n",
    "miniparCol = defaultdict(lambda: defaultdict(lambda: Counter()))\n",
    "pw = defaultdict(lambda: defaultdict(lambda:Counter()))\n",
    "pw_ratio = defaultdict(lambda: defaultdict(lambda:Counter()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_DB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictDet = {'some' :\"<br>Some</br> is usually used to show that there is a quantity of something or a number of things or people, without being precise. It is used with uncountable nouns and plural countable nouns.\",\n",
    "'a2some' : \"When you want to emphasize that you do not know the identity of a person or thing, or you think their identity is not important, you can use <br>some</br> with a sigular countable noun, instead of a or an.\",\n",
    "'any' : \"<br>Any</br> is used before pluarl nouns and uncountable nouns when you are refferring to or asking whether a quantity of something that may or may not exist.\",\n",
    "'another' : \"<br>Another</br> is used with singular countable nouns to talk about an additional person or thing of the same type as you have already mentioned.\",\n",
    "'other' : \"<br>Other</br> is used with plural nouns, or occasionally with uncountable nouns.\",\n",
    "'enough' : \"<br>Enough</br> is used to say that there is as much of something as is needed, or as many things as are needed. You can therefore use enough in front of uncountable nounds or plural nouns.\",\n",
    "'few' : \"When you want to emphasize that there is only a samll number of things of a particular kind, you use <br>few</br> with a plural countable noun.\",\n",
    "'many' : \"<br>Many</br> indicates that there is a large number of things, without being very precise. You use <br>many</br> with a plural countable noun.\",\n",
    "'most' : \"<br>Most</br> indicates nearly all of a group or amount. You use <br>most</br> with an uncountable noun or a plural countable noun.\",\n",
    "'several' : \"<br>Several</br> usually indicates an imprecise number that is not very large, but it is more than two. You use <br>several</br> with a plural countable noun.\",\n",
    "'all' : \"<br>All</br> includes every person or thing of a particaular kind. You use <br>all</br> with an uncountable noun or a plural countable noun.\",\n",
    "'both' : \"<br>Both</br> is used to say something about two people or things of the same kind. You use both with a plural countable noun.\" ,\n",
    "'either' : \"<br>Either</br> is used to talk about two things, but usaully indicates that only one of the two is invloved. You use either with a singular countable noun.\",\n",
    "'each' : \"<br>Each</br> is used when you are thinking about the members as individuals. You use <br>each</br> with a singular countable noun.\",\n",
    "'every' : \"<br>Every</br> is used when you are making a general statement about all of them. You use <br>every</br> with a singular countbale noun.\",\n",
    "'little' : \"<br>Little</br> is used to emphasize only a small amount of something. You use <br>little</br> with uncountable nouns.\",\n",
    "'much' : \"<br>Much</br> is used to emphasize a large amount. You use <br>much</br> with uncountable nouns.\",\n",
    "'this' : \"<br>This</br> is used to talk about people or things that are very obvious in the situation that you are in.\",\n",
    "'these' :  \"<br>There</br> is used to talk about people or things that are very obvious in the situation that you are in. You use <br>these</br> with a plural countable noun.\",\n",
    "'that' : \"<br>That</br> is used to  talk about people or things that are you can see but that are not very cloased to you.\",\n",
    "'those' : \"<br>Those</br> is used to  talk about people or things that are you can see but that are not very cloased to you. You use <br>those</br> with a plural countable noun.\",\n",
    "'the':\"<br>The</br> is used before a noun when <br>a</br> has been mentioned or nouns are sprecific names or proper nouns.\",\n",
    "'a':\"<br>A<br> is used for talking about a person or thing when it is not important or not clear.\",\n",
    "'an':\"<br>An<br> is used for talking about a person or thing when it is not important or not clear.\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbpat =  ['V that', 'V n that', 'V n', 'V pron-refl', 'V n of n', 'V n to inf', 'V in n', 'V wh', 'V adv',\\\n",
    "'V n from n', 'V pron-refl with n', 'V pron-refl by n', 'V n adj', 'V for n', 'V n in n', \\\n",
    "'V to inf', 'V n for n', 'V to n', 'V on n', 'be V-ed in n', 'be V-ed as n', 'be V-ed with n',\\\n",
    "'V with n', 'be V-ed on n', 'V n with n', 'V through n', 'V n by n', 'V into n',\\\n",
    "'V across n', 'V around n', 'V n on n', 'V -ing', 'V from n', 'V after n', 'V n to n', \\\n",
    "'V pron-refl to n', 'V at n', 'V n at n', 'V n n', 'V n into n', 'be V-ed of n', 'V of n',\\\n",
    "'V n against n', 'V by n', 'V pron-refl out', 'V n as n', 'V over n', 'V n -ing', 'V n through n',\\\n",
    "'V against n', 'V adj', 'be V-ed of', 'V n out', 'V n up', 'V about n', 'V that n inf', \\\n",
    "'V pron-refl up', 'be V-ed to n', 'be V-ed n', 'be V-ed adj', 'V inf', 'V n inf', 'V n about n', \\\n",
    "'V n over n', 'V toward n', 'V towards n', 'be V-ed against n', 'be V-ed at n', 'V off n', \\\n",
    "'V behind n', 'V pron-refl on n', 'V pron-refl for n', 'V out of n', 'be V-ed amount', \\\n",
    "'be V-ed to', 'V under n', 'be V-ed into n', 'V amount', 'be V-ed with', 'V pron-refl in n', \\\n",
    "'V among n', 'V as n', 'V n off', 'V pron-refl at n', 'be V-ed from n', 'be V-ed for n', 'V n under n',\\\n",
    "'V pron-refl from n', 'V pron-refl off', 'be V-ed across n', 'V n without n', 'V n toward n', \\\n",
    "'V between n', 'be V-ed over n', 'be V-ed to inf', 'V n after n', 'V n across n', 'be V-ed against', \\\n",
    "'V n down', 'be V-ed as', 'be V-ed for', 'V pron-refl into n', 'V n off n', 'V without n', 'be V-ed off n', \\\n",
    "'V pron-refl of n', 'be V-ed about n', 'be V-ed that', 'V n among n', 'be V-ed about', \\\n",
    "'V pron-refl against n', 'V n between n', 'V n around n', 'V pron-refl as n', 'be V-ed by', 'be V-ed after n',\\\n",
    "'be V-ed by n', 'be V-ed on', 'be V-ed among n', 'be V-ed between n', 'be V-ed at',\\\n",
    "'be V-ed around n', 'V n behind n', 'V pron-refl down', 'be V-ed in', 'be V-ed under n', \\\n",
    "'be V-ed without n', 'be V-ed through', 'be V-ed after', 'be V-ed through n', 'V n towards n',\\\n",
    "'V pron-refl off n', 'V pron-refl between n']+\\\n",
    "['V wh n','V n wh','V wh to inf']\n",
    "verbpat.extend([v.replace('wh',tar) for v in set([v for v in verbpat if 'wh' in v]) for tar in ['how' , 'who' , 'what', 'when','why','where']])\n",
    "verbpat = set(verbpat)\n",
    "nounpat = ['adj N', 'with N', 'from N', 'n N', 'to N', '(v) N in n', 'into N', 'v N with n', \\\n",
    "               '(v) N of n', 'amount N', '(v) N to n', 'in N', '(v) N on n', 'on N', 'at N', 'N of n', \\\n",
    "               '(v) N into n', 'N as n', '(v) N with n', '(v) N from n', '(v) N for n', 'N in n', 'v N over n', \\\n",
    "               'N for -ing', 'N to inf', 'N to n', 'v N in n', 'without N', 'N from n', 'v N about n', 'under N', \\\n",
    "               'N to -ing', 'N on n', 'v N from n', 'v N between n', '(v) N over n', 'N for n', 'in N of n', \\\n",
    "               '(v) N through n', '(v) N toward n', '(v) N against n', '(v) N towards n', 'N that', 'on N of n',\\\n",
    "               '(v) N at n', '(v) N between n', 'v N by n', 'v N through n', '(v) N around n', 'v N for n', \\\n",
    "               '(v) N about n', 'v N on n', 'v N of n', 'v N without n', 'N with n', '(v) N among n', 'N by n', \\\n",
    "               'N about n', 'N through n', '(v) N behind n', '(v) N as n', 'v N as n', 'N among n', '(v) N by n', \\\n",
    "               'v N against n', 'N about -ing', 'N around n', 'v N across n', 'N at n', 'N between n', 'v N to n', \\\n",
    "               'N over n', 'N toward n', 'v N at n', 'N in -ing', 'v N into n', 'v N off n', 'N against n', 'N into n',\\\n",
    "               'N across n', 'v N toward n', 'N under n', 'v N after n', 'v N among n', 'v as N', 'v N under n', \\\n",
    "               '(v) in N', 'N on -ing', 'N behind n', '(v) N across n', 'v N around n', '(v) N under n', 'N from -ing', 'v N out n'] + \\\n",
    "             ['N %s wh to inf'% pg for pg in ['in','of','on']]\n",
    "nounpat.extend([v.replace('wh',tar) for v in set([v for v in nounpat if 'wh' in v]) for tar in ['how' , 'who' , 'what', 'when','why','where']])\n",
    "nounpat = set(nounpat)\n",
    "adjpat = ['ADJ n', 'ADJ to n', 'ADJ and adj', 'n ADJ', 'ADJ that', 'ADJ in n', 'ADJ about n', 'n N', 'N at n', \\\n",
    "             'ADJ for n', 'ADJ with n', 'ADJ to inf', 'ADJ on n', 'ADJ as n', 'adj N', 'ADJ into n', 'ADJ of n', \\\n",
    "             'amount N', 'ADJ toward n', 'N to n', 'amount ADJ', 'ADJ from n', 'ADJ at n', 'N of n', 'ADJ by n',\\\n",
    "             'ADJ against n', 'ADJ among n', 'ADJ in n with n', 'ADJ -ing', 'ADJ through n', 'N from n', 'ADJ over n',\\\n",
    "             'ADJ wh', 'ADJ without n', 'ADJ under n', 'N on n', 'N in n', 'of N', 'N with n', 'ADJ between n', 'N as n',\\\n",
    "             'N among n', 'ADJ on n for n', 'N by n', 'ADJ in n from n', 'ADJ to n for n', 'ADJ after n', 'N behind n', \\\n",
    "             'N through n', 'ADJ around n', 'N over n', 'N for n', 'N after n', 'ADJ across n']\n",
    "\n",
    "selfWords = {'oneself', 'myself', 'ourselves', 'yourself', 'himself', 'herself', 'themselves','me','him','you','her','them','it'}\n",
    "pgPreps = 'under|without|around|round|in_favor_of|_|about|after|against|among|as|at|between|behind|by|for|from|in|into|of|on|upon|over|through|to|toward|forward|off|on|across|towards|with|out'.split('|')\n",
    "otherPreps ='out|off|down|up|across'.split('|')\n",
    "reserveWord = {'for', 'over', 'at', 'about', 'up', 'by', 'under', 'among', 'on', 'out', 'that', 'against', 'of', 'in', 'amount', 'to', 'between', 'toward', 'towards', 'down', 'from', 'as', 'through', 'around', 'and', 'off', 'into', 'without', 'with', 'after', 'across', 'behind'}\n",
    "allreserved = set()\n",
    "allreserved = allreserved.union(set(pgPreps) , set(otherPreps) , reserveWord)\n",
    "pos_map = {'N':'N','J':'ADJ','V':'V','A':'ADJ'}\n",
    "det_s = set(\"a,an,this,that,each,every,either,another,the,no\".split(','))\n",
    "det_p = set(\"the,some,these,those,much,many,any,all,most,enough,several,other,few,both\".split(','))\n",
    "MONTH = set(\"january,february,march,april,may,june,july,august,september,october,november,december\".split(','))\n",
    "WEATHER = set(\"spring,summer,fall,autumn,winter\".split(\",\"))\n",
    "DATES = set(\"monday,tuesday,wednesday,thursday,friday,saturday,sunnday\".split(\",\"))\n",
    "HOLIDAY = set(\"christmas,easter,hannukkah,ramadan\".split(\",\"))\n",
    "CLOCKTIME = set(\"midnight/noon/dawn/lunch\".split('/'))\n",
    "POD = set(\"morning/afternoon/evening\".split(\"/\"))\n",
    "mapHead = dict( [('H-NP', 'N'), ('H-VP', 'V'), ('H-ADJP', 'ADJ'), ('H-ADVP', 'ADV'), ('H-VB', 'V')] )\n",
    "mapRest = dict( [('VBG', '-ing'), ('VBD', 'v-ed'), ('VBN', 'v-ed'), ('VB', 'v'), ('NN', 'n'), ('NNS', 'n'), ('JJ', 'adj'), ('RB', 'adv'),('NP', 'n'), ('VP', 'v'), ('JP', 'adj'), ('ADJP', 'adj'), ('ADVP', 'adv'), ('SBAR', 'that')] )\n",
    "modeMap = {'V':'V','J':'ADJ','N':'N'}\n",
    "vowel = set([i for i in 'aeiouh'])\n",
    "vowelMap = {'a':['apple','apartment'],'e':['elephant','element'],'i':['igloo','island'],'o':['oven','octopus'],'u':['umbrella','unexpected error'],'h':['hour','honour']}\n",
    "TIME = set(list(MONTH)+\\\n",
    "                list(WEATHER)+\\\n",
    "                list(DATES)+\\\n",
    "                list(HOLIDAY)+\\\n",
    "                list(CLOCKTIME)+\\\n",
    "                list(POD)+\\\n",
    "                ['weekend','weekends'])\n",
    "maxDegree = 9\n",
    "tmp_abbrs = {'it’s':'it is','what’s':'what is','how’s':'how is','they’re':'they are','we’re':'we are','i’m':'I am','don’t':'do not','doesn’t':'does not','didn’t':'did not','won’t':'will not','hadn’t':'had not','haven’t':'have not','wouldn’t':'would not','couldn’t':'could not','can’t':'can not','shouldn’t':'should not'}\n",
    "abbrs = defaultdict()\n",
    "for key,val in tmp_abbrs.items():\n",
    "    abbrs[key.replace(\"’\",\"'\")] = val\n",
    "    abbrs[key] = val\n",
    "aux = set([('will','would'),('can','could'),('shall','should'),('may','might')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rephrase(sent):\n",
    "    words = []\n",
    "    for s in word_tokenize(sent):\n",
    "        if not s in set('[|]|{|}'.split('|')):\n",
    "            if s[0]== '+' and s[-1] =='+':\n",
    "                words.append(s[1:-1])\n",
    "            elif s[0]=='-' and s[-1] == '-':\n",
    "                continue\n",
    "            else:\n",
    "                words.append(s)\n",
    "    return words\n",
    "\n",
    "def simplify(pat):\n",
    "    i=0\n",
    "    for p in pat[::-1]:\n",
    "        if p=='adv':\n",
    "            i-=1\n",
    "        else:\n",
    "            break\n",
    "    return pat[:i]\n",
    "\n",
    "def sentence_to_ngram(words, lemmas, tags, chunks): \n",
    "    return [ (k, k+degree) for k in range(0,len(words)) for degree in range(1, min(maxDegree, len(words)-k+1)) ]\n",
    "\n",
    "def hasTwoObjs(tag, chunk):\n",
    "    if chunk[-1] != 'H-NP': return False\n",
    "    return (len(tag) > 1 and tag[0] in pronOBJ) or (len(tag) > 1 and 'DT' in tag[1:])\n",
    "def chunk_to_element(words, lemmas, tags, chunks, i, isHead):\n",
    "    if isHead:\n",
    "        if len(chunks[i][-1])>3:\n",
    "            # print('chunk',chunks[i][-1])\n",
    "            if lemmas[i-1][-1] =='be' and tags[i][0]=='VBN': return 'V-ed'\n",
    "            elif tags[i][-1][0] in ['V','N']:\n",
    "                return tags[i][-1][0] \n",
    "            elif tags[i][-1][0]=='J':\n",
    "                return 'ADJ'\n",
    "            elif lemmas[i][-1] in pgPreps:\n",
    "                return lemmas[i][-1]\n",
    "    if tags[i][-1] == 'DT': return \"\"\n",
    "    if lemmas[i-1][-1]==\"to\" and tags[i][0]=='VB': return 'inf'\n",
    "    if lemmas[i][-1].lower() in reserveWord: return lemmas[i][0].lower() \n",
    "    if lemmas[i][-1].lower()  == 'be': return 'be'\n",
    "    if lemmas[i][-1].lower()  in ['how' , 'who' , 'what', 'when','why','where'] : return lemmas[i][-1].lower()\n",
    "    if chunks[i][0][2:5]=='ADV': return 'adv'\n",
    "    if chunks[i][0]=='I-ADJP': return 'adj'\n",
    "    if lemmas[i][-1] in selfWords : return 'pron-refl'\n",
    "    if tags[i-1][0]=='V' and lemmas[i][0]=='VB': return 'inf'\n",
    "    if  lemmas[i-2][0]+\" \"+tags[i][0]== \"that N\" and lemmas[i][0] =='VB': return 'inf'\n",
    "    if tags[i][0] == 'VBG': return '-ing'\n",
    "    if tags[i][0] == 'CD': return 'amount'\n",
    "    if tags[i][0]=='J': return 'adj'\n",
    "    if tags[i][0]=='V': return 'v'\n",
    "    if tags[i][0]=='N': return 'n'\n",
    "    if tags[i][0]=='PRP': return 'n'\n",
    "    if tags[i][0] == 'PRP$': return 'adj'\n",
    "    if lemmas[i][0] == 'favour' and words[i-1][-1]=='in' and words[i+1][0]=='of': return 'favour'\n",
    "    if tags[i][-1] == 'RP' and tags[i-1][-1][:2] == 'VB':                return '_'\n",
    "    if tags[i][0]=='CD': return 'amount'\n",
    "    if hasTwoObjs(tags[i], chunks[i]):                                              return 'n n'\n",
    "    if tags[i][-1] in mapRest:                            return mapRest[tags[i][-1]]\n",
    "    if tags[i][-1][:2] in mapRest:                        return mapRest[tags[i][-1][:2]]\n",
    "    if chunks[i][-1] in mapHead:                            return mapHead[chunks[i][-1]].lower()\n",
    "    if lemmas[i][-1] in pgPreps:                                         return lemmas[i][-1]\n",
    "    return lemmas[i][-1]\n",
    "\n",
    "def simplifyPat(pat): \n",
    "    if pat == 'V ,':\n",
    "        return 'V'\n",
    "    elif pat =='N ,':\n",
    "        return 'N'\n",
    "    elif pat =='J ,':\n",
    "        return 'ADJ'\n",
    "    else:\n",
    "        return pat.replace(' _', '').replace('_', ' ').replace('  ', ' ')\n",
    "\n",
    "def ngram_to_pat(words, lemmas, tags, chunks, start, end):\n",
    "    pat, doneHead = [], False\n",
    "    head_pos = start\n",
    "    change_start = False\n",
    "    for i in range(start, end):\n",
    "        isHead = tags[i][-1][0] in ['V','J','N'] and not doneHead\n",
    "        if isHead:\n",
    "            if tags[i][-1][0]=='V':\n",
    "                if lemmas[i][-1] =='be' and tags[i+1][0]=='VBN':\n",
    "                    isHead = False\n",
    "            elif tags[i][-1][0]=='N':\n",
    "                if i>0:\n",
    "                    if tags[i-1][-1][0]=='V': pat.append('(v)')\n",
    "                    if tags[i-1][-1][0]=='J': \n",
    "#                         change_start = True\n",
    "                        pat.append('adj')\n",
    "            else:\n",
    "                isHead = not lemmas[start][-1].lower() in pgPreps\n",
    "        pat.append( chunk_to_element(words, lemmas, tags, chunks, i, isHead) )\n",
    "        if isHead: doneHead = True\n",
    "        else:\n",
    "            if not doneHead: head_pos+=1\n",
    "\n",
    "    pat = simplifyPat(' '.join(pat))\n",
    "    tmp_pat= []\n",
    "    for p in pat.split():\n",
    "        if not tmp_pat:\n",
    "            tmp_pat.append(p)\n",
    "        else:\n",
    "            if p!=tmp_pat[-1]:\n",
    "                tmp_pat.append(p)\n",
    "    if head_pos<end:\n",
    "        pat = pat.replace('adj n','n')\n",
    "        if isverbpat(lemmas[head_pos][0],pat):\n",
    "            mode = 'V'\n",
    "            return pat,head_pos,change_start\n",
    "        elif isverbpat(lemmas[head_pos][0],pat.replace('wh','n')):\n",
    "            mode = 'V'\n",
    "            return pat,head_pos,change_start\n",
    "        elif isverbpat(lemmas[head_pos][0],pat.replace('and','').replace('adj n','n')):\n",
    "            mode = 'V'\n",
    "            return pat.replace('and','').replace('adj n','n'),head_pos,change_start\n",
    "        elif isverbpat(lemmas[head_pos][0],pat.replace('and','').replace('adj n','n')):\n",
    "            mode = 'V'\n",
    "            return pat.replace('and','').replace('adj n','n'),head_pos,change_start\n",
    "        elif isverbpat(lemmas[head_pos][0],pat.replace('pron-refl','n').replace('adj n','n')):\n",
    "            mode = 'V'\n",
    "            return pat.replace('adj n','n'),head_pos,change_start\n",
    "        elif isnounpat(lemmas[head_pos][0],pat):\n",
    "            mode = 'N'\n",
    "            return pat,head_pos,change_start\n",
    "        elif isnounpat(lemmas[head_pos][0],pat.replace('-ing','n')):\n",
    "            mode = 'N'\n",
    "            return pat,head_pos,change_start\n",
    "        elif isnounpat(lemmas[head_pos][0],pat.replace('wh','n')):\n",
    "            mode = 'N'\n",
    "            return pat,head_pos,change_start\n",
    "        elif isnounpat(lemmas[head_pos][0],pat.replace('amount N','n N')):\n",
    "            mode = 'N'\n",
    "            return pat[4:].replace('adv','').replace('amount N','n N'),head_pos,change_start\n",
    "        elif isnounpat(lemmas[head_pos][0],pat.replace('amount N','N')):\n",
    "            mode = 'N'\n",
    "            return pat[4:].replace('amount N','N'),head_pos,change_start\n",
    "        elif isnounpat(lemmas[head_pos][0],pat.replace('amount N','N')):\n",
    "            mode = 'N'\n",
    "            return pat.replace('adv','').replace('amount N','N'),head_pos,change_start\n",
    "        elif pat[:3] =='(n)' and isnounpat(lemmas[head_pos][0],pat[4:].replace('amount N','n N')):#(n)\n",
    "            mode = 'N'\n",
    "            return pat[4:].replace('amount N','n N'),head_pos,change_start\n",
    "        elif isnounpat(lemmas[head_pos][0],pat.replace('adj N','N')):\n",
    "            mode = 'N'\n",
    "            return pat.replace('adj N','N'),head_pos,change_start\n",
    "        elif isnounpat(lemmas[head_pos][0],pat.replace('what','n')):\n",
    "            mode = 'N'\n",
    "            return pat.replace('what','n'),head_pos,change_start\n",
    "        elif isadjpat(lemmas[head_pos][0],pat):\n",
    "            mode = 'ADJ'\n",
    "            return pat,head_pos,change_start\n",
    "        elif isadjpat(lemmas[head_pos][0],pat.replace('-ing','n')):\n",
    "            mode = 'ADJ'\n",
    "            return pat,head_pos,change_start\n",
    "        elif isadjpat(lemmas[head_pos][0],pat.replace('wh','n')):\n",
    "            mode = 'ADJ'\n",
    "            return pat,head_pos,change_start\n",
    "        elif isadjpat(lemmas[head_pos][0],pat.replace('amount N','n N')):\n",
    "            mode = 'ADJ'\n",
    "            return pat[4:].replace('amount N','n N'),head_pos,change_start\n",
    "        elif isadjpat(lemmas[head_pos][0],pat.replace('amount N','N')):\n",
    "            mode = 'ADJ'\n",
    "            return pat[4:].replace('amount N','N'),head_pos,change_start\n",
    "        elif pat[:3] =='(n)' and isadjpat(lemmas[head_pos][0],pat[4:].replace('amount N','n N')):#(n)\n",
    "            mode = 'ADJ'\n",
    "            return pat[4:].replace('amount N','n N'),head_pos,change_start\n",
    "    return \"\" ,start,change_start\n",
    "\n",
    "def isverbpat(key,pat):\n",
    "    pat = ' '.join(pat.split())\n",
    "    return  pat in verbpat\n",
    "\n",
    "def isnounpat(key,pat):\n",
    "    pat = ' '.join(pat.split())\n",
    "    return pat in nounpat\n",
    "\n",
    "def isadjpat(key,pat):\n",
    "    pat = ' '.join(pat.split())\n",
    "    return pat in adjpat\n",
    "\n",
    "def ngram_to_head(words, lemmas, tags, chunks, start, end,real_start):\n",
    "    for i in range(start, end):\n",
    "        if tags[i][-1][0] in ['V','N','J']:  \n",
    "            return modeMap[tags[i][-1][0]],lemmas[i][-1].upper(),words[real_start:end]\n",
    "    return \"\",\"\"\n",
    "\n",
    "def geniatag(line):\n",
    "    taggers = tagger.parse(line)\n",
    "    a = []\n",
    "    b = []\n",
    "    c = []\n",
    "    d = []\n",
    "    tmp = []\n",
    "    for parse in taggers:\n",
    "        a.append([parse[0]])\n",
    "        b.append([parse[1]])\n",
    "        c.append([parse[2]])\n",
    "        d.append([parse[3]])\n",
    "    tmp.append(a)\n",
    "    tmp.append(b)\n",
    "    tmp.append(c)\n",
    "    tmp.append(d)\n",
    "    return tmp\n",
    "\n",
    "def find_patterns(parse,target):\n",
    "    tmp = []\n",
    "    for start, end in sentence_to_ngram(*parse):\n",
    "        pat, head_pos,change_start = ngram_to_pat(*parse, start, end) \n",
    "        if pat:\n",
    "            if change_start:\n",
    "                start -= 1\n",
    "            mode,head,sent = ngram_to_head(*parse, head_pos, end,start)\n",
    "            pat = ' '.join(pat.split())\n",
    "            head = head.lower()\n",
    "            pat_example = ' '.join([s[0] for s in sent])\n",
    "            whs = [wh for wh in ['how' , 'who' , 'what', 'when','why','where'] if wh in pat]\n",
    "            if whs:\n",
    "                for wh in whs:\n",
    "                    tmp.append([pat.replace(wh,'wh'),head,mode,pat_example])\n",
    "            if any([t for t in target if re.findall(r'\\b' + re.escape(t)+r'\\b', pat_example)]):\n",
    "#                 if any([pat[0].split(\"%\")[0] for pat in dictWord[mode][head]]):\n",
    "                    tmp.append([pat,head,mode,pat_example])\n",
    "    return tmp\n",
    "\n",
    "def find_phrases(patterns,head,part):\n",
    "    output_phrases = []\n",
    "    for pattern in patterns:\n",
    "        words = word_tokenize(pattern)\n",
    "        words = ' '.join([w for w in words if w in allreserved or w==part]).replace(part,head)\n",
    "        output_phrases.extend([phrase for phrase in phraseV[head].keys() if phrase.split('%')[0] == words])\n",
    "    return set(output_phrases)\n",
    "\n",
    "def find_meaning(lista,listb,a,b):\n",
    "    worda = []\n",
    "    wordb = []\n",
    "#     print(list(dicta.values()))\n",
    "    if not [t[1].strip() for t in lista if t[1].strip()] :\n",
    "        worda.append(a)\n",
    "    else:\n",
    "        worda = [la[1].replace('SOMETHING','').replace('THING','').strip() for la in lista if la[1].replace('SOMETHING','').replace('THING','')]\n",
    "    if not [t[1].strip() for t in listb if t[1].strip()]:\n",
    "        wordb.append(b)\n",
    "    else:\n",
    "        wordb = [la[1].replace('SOMETHING','').replace('THING','').strip() for la in listb if la[1].replace('SOMETHING','').replace('THING','')]\n",
    "   \n",
    "    index, _ = max(enumerate( [nlp(wa.lower())[0].similarity(nlp(wb.lower())[0]) for wa in worda for wb in wordb]), key=operator.itemgetter(1))\n",
    "    if len(lista[int(index/len(wordb))][2][0]) == 2:\n",
    "        eng = lista[int(index/len(wordb))][2][0][0]\n",
    "        ch = \" ( \"+lista[int(index/len(wordb))][2][0][1] +\" )\"\n",
    "    else:\n",
    "        eng = lista[int(index/len(wordb))][2][0][1]\n",
    "        ch = \" ( \"+lista[int(index/len(wordb))][2][0][2] +\" )\"\n",
    "    if braces.search(eng):\n",
    "            eng = eng.replace(braces.search(eng)[0],'') \n",
    "    attra = eng+ch\n",
    "    \n",
    "    if len(listb[index%len(wordb)][2][0]) == 2:\n",
    "        eng = listb[index%len(wordb)][2][0][0]\n",
    "        ch = \" ( \"+ listb[index%len(wordb)][2][0][1]+\" )\"\n",
    "    else:\n",
    "        eng = listb[index%len(wordb)][2][0][1]\n",
    "        ch = \" ( \"+ listb[index%len(wordb)][2][0][2]+\" )\"\n",
    "    if braces.search(eng):\n",
    "            eng = eng.replace(braces.search(eng)[0],'')\n",
    "    attrb = eng + ch\n",
    "    return attra,attrb\n",
    "\n",
    "def explain_voc_semantic_error(correction,d_lemma,d_part,a_lemma,a_part):\n",
    "    output = []\n",
    "    output.append('It is a semantic error.')\n",
    "    listd = dictDef[d_lemma][d_part.upper()]\n",
    "    lista = dictDef[a_lemma][a_part.upper()]\n",
    "    if not listd:\n",
    "        listd = dictDef[d_lemma]['']\n",
    "    if not lista:\n",
    "        lista = dictDef[a_lemma]['']\n",
    "    if listd and lista:\n",
    "        delmeaning,addmeaning = find_meaning(listd,lista,d_lemma,a_lemma)\n",
    "        if delmeaning:\n",
    "            output.append('<b>%s</b> :\\t%s.'%(d_lemma,delmeaning))\n",
    "        if addmeaning:\n",
    "            output.append(\"<b>%s</b> :\\t%s.\"%(a_lemma,addmeaning))\n",
    "        \n",
    "    return '<p>'+'</p><p>'.join(output)+'</p>'\n",
    "\n",
    "def explain_VT_error(head,correction,pattern,ex):\n",
    "    output = []\n",
    "    head = head.lower()\n",
    "    if dictDef[head]:\n",
    "        Ts = []\n",
    "        Is = []\n",
    "        emp = []\n",
    "        for  df in dictDef[head]['V']:\n",
    "            if 'T' in df[0]:\n",
    "                Ts.append(merge_def(df))\n",
    "            elif 'I' in df[0]:\n",
    "                Is.append(merge_def(df))\n",
    "            elif not df[0]:\n",
    "                emp.append(merge_def(df))\n",
    "        if Ts and not Is and not emp:\n",
    "            output.append(\"For all the situations, the verb <b>%s</b> is not followed by a preposition <b>%s</b> because <b>%s</b> is a transitive verb.\"%(head,deletion.search(correction).group(1),head))\n",
    "        elif Ts and Is:\n",
    "            output.append(\"The verb <b>%s</b> can be both transitive and intransitive verb. When it means %s, it is a transitive verb. However, when it describes that %s, it represents a intransitive verb.\"%(head,' or '.join(Ts[:2]),' or '.join(Is[:2])))\n",
    "        elif emp:\n",
    "            output.append(\"Normally, the verb <b>%s</b> is not followed by a preposition <b>%s</b> because <b>%s</b> is a transitive verb. Besides, it means %s.\"%(head,deletion.search(correction).group(1),head,emp[0]))\n",
    "        output.append(explain_pattern(ex,head,'V',pattern))\n",
    "    else:\n",
    "        output.append(\"Normally, the verb <b>%s</b> is not followed by a preposition <b>%s</b> because <b>%s</b> is a transitive verb.\"%(head,deletion.search(correction).group(1),head))\n",
    "        output.append(explain_pattern(ex,head,'V',pattern))\n",
    "    return '<p>'+'</p><p>'.join(output)+'</p>'\n",
    "        \n",
    "def explain_VI_error(head,correction,pattern,ex):\n",
    "    output = []\n",
    "    head = head.lower()\n",
    "    if dictDef[head]:\n",
    "        Ts = []\n",
    "        Is = []\n",
    "        emp = []\n",
    "        for  df in dictDef[head]['V']:\n",
    "            if 'I' in df[0]:\n",
    "                Is.append(merge_def(df))\n",
    "            if 'T' in df[0]:\n",
    "                Ts.append(merge_def(df))\n",
    "                \n",
    "        if Is and not Ts:\n",
    "            output.append(\"The verb <b>%s</b> is absolutely an intransitive verb and it means %s.\"%(head,' or '.join(Is[:2])))\n",
    "        elif Ts:\n",
    "            output.append(\"The verb <b>%s</b> is an intransitive verb here which means %s. However, it can be transitive sometimes depending on the scenerio.\"%(head,' or '.join(Is[:2])))\n",
    "        else:\n",
    "            output.append(\"Normally, the verb <b>%s</b> is necessarily followed by a preposition <b>%s</b> because <b>%s</b> is a intransitive verb.\"%(head,addition.search(correction).group(1),head))\n",
    "            output.append(explain_pattern(ex,head,'V',pattern))\n",
    "    return '<p>'+'</p><p>'.join(output)+'</p>'\n",
    "\n",
    "            \n",
    "def merge_def(df):\n",
    "    defsent = ' '.join(df[2][0][:-1]).strip()\n",
    "    defsent += ' ( '+df[2][0][-1].strip()+' )'\n",
    "    if braces.search(defsent):\n",
    "        idx = defsent.find(braces.search(defsent).group(0))\n",
    "        if idx == 0:\n",
    "            defsent = defsent.replace(braces.search(defsent).group(0),\"\").strip()\n",
    "            return defsent\n",
    "        else:\n",
    "            return defsent\n",
    "    else:\n",
    "        return defsent\n",
    "    \n",
    "def find_N_meaning(head,isUncount = False):\n",
    "    output = []\n",
    "    head = head.lower()\n",
    "    if dictDef[head]:\n",
    "        Cs = []\n",
    "        Us = []\n",
    "        emp = []\n",
    "        for  df in dictDef[head]['N']:\n",
    "            part_ = [d.strip() for d in ' '.join([braces.search(d).group(1) for d in df[2][0] if braces.search(d)]).split('or') if d.strip()]\n",
    "            if isUncount:\n",
    "                if 'U' in part_:\n",
    "                    def_sent = merge_def(df)\n",
    "                    if def_sent:\n",
    "                        Us.append(def_sent)\n",
    "                elif 'C' in part_:\n",
    "                    def_sent = merge_def(df)\n",
    "                    if def_sent:\n",
    "                        Cs.append(def_sent)\n",
    "                elif 'PLURAL' in part_:\n",
    "                    def_sent = merge_def(df)\n",
    "                    if def_sent:\n",
    "                        Cs.append(def_sent)\n",
    "                elif not part_:\n",
    "                    def_sent = merge_def(df)\n",
    "                    if def_sent:\n",
    "                        emp.append(def_sent)\n",
    "            else:\n",
    "                if 'C' in part_:\n",
    "                    def_sent = merge_def(df)\n",
    "                    if def_sent:\n",
    "                        Cs.append(def_sent)\n",
    "                elif 'U' in part_:\n",
    "                    def_sent = merge_def(df)\n",
    "                    if def_sent:\n",
    "                        Us.append(def_sent)\n",
    "                elif 'PLURAL' in part_:\n",
    "                    def_sent = merge_def(df)\n",
    "                    if def_sent:\n",
    "                        Cs.append(def_sent)\n",
    "                elif not part_:\n",
    "                    def_sent = merge_def(df)\n",
    "                    if def_sent:\n",
    "                        emp.append(def_sent)\n",
    "#         print(Cs,Us,emp)\n",
    "        if isUncount:\n",
    "            if Us and not Cs:\n",
    "                output.append(\"The noun <b>%s</b> is uncountable all the time. It means %s which describes relatively abstract concept.\"%(head,' or '.join(Us[:2])))\n",
    "            elif Us and Cs:\n",
    "                output.append(\"The noun <b>%s</b> can be both countable and uncountable which depends on its definition. When it means %s, it expresses an abstract concept so it is uncountable for sure. However, when it explians %s, it represents a countable noun.\"%(head,' or '.join(Us),' or '.join(Cs)))\n",
    "            elif emp:\n",
    "                output.append(\"The noun <b>%s</b> is uncountable because it descibes relatively abstract concept that <b>%s</b>.\"%(head,' or '.join(emp[:2])))\n",
    "        else:\n",
    "            if Cs and not Us:\n",
    "                output.append(\"The noun <b>%s</b> is countable all the time. It means %s which describes relatively abstract concept.\"%(head,' or '.join(Cs[:2])))\n",
    "            elif Us and Cs:\n",
    "                output.append(\"The noun <b>%s</b> can be both countable and uncountable which depends on its definition. When it means %s, it expresses an abstract concept so it is uncountable for sure. However, when it explians %s, it represents a countable noun.\"%(head,' or '.join(Us),' or '.join(Cs)))\n",
    "            elif emp:\n",
    "                output.append(\"The noun <b>%s</b> is countable when it means <b>%s</b>.\"%(head,' or '.join(emp[:2])))\n",
    "    return '<p>'+'</p><p>'.join(output)+'</p>'\n",
    "\n",
    "def check_uncountable(head):\n",
    "    head = head.lower()\n",
    "    if dictDef[head]:\n",
    "        for  df in dictDef[head]['N']:\n",
    "            part_ = [d.strip() for d in ' '.join([braces.search(d).group(1) for d in df[2][0] if braces.search(d)]).split('or') if d.strip()]\n",
    "            if 'U' in part_:\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def my_lemma(correction,entails_sent):\n",
    "    delset = []\n",
    "    delword = []\n",
    "    if deletion.search(correction):\n",
    "        head = deletion.search(correction).group(1)\n",
    "        entails_sent = correction.replace(deletion.search(correction).group(0),head)\n",
    "        tags = geniatag(entails_sent)\n",
    "    else:\n",
    "        head = addition.search(correction).group(1)\n",
    "        tags = geniatag(' '.join(entails_sent))\n",
    "    idx = [t[0] for t in tags[0]].index(head)\n",
    "    \n",
    "    return tags[1][idx][0],tags[2][idx][0]\n",
    "\n",
    "def find_voc_meaning(head,part):\n",
    "    output = []\n",
    "    part = part.upper()\n",
    "    mapp_ = {'V':'verb','N':'noun','ADJ':'adjective','C':'countable','U':'uncountable','PLURAL':'usually in plural format','I':'intransitive','T':'transitive'}\n",
    "    head = head.lower()\n",
    "    if dictDef[head] and dictDef[head][part]:\n",
    "#         find alternative part description\n",
    "        is_mapp_ = set([mapp_[ex] for exs in dictDef[head][part] for ex in exs[0] if ex in mapp_])\n",
    "#         output.append(head,part,dictDef[head][part][0][2])\n",
    "        eng = dictDef[head][part][0][2][0][0].strip()\n",
    "        if braces.search(eng):\n",
    "            eng = eng.replace(braces.search(eng)[0],'')\n",
    "        ch = '( '+dictDef[head][part][0][2][0][1] + ' )'\n",
    "        if is_mapp_:\n",
    "            if 'uncountable' in is_mapp_:\n",
    "                output.append(\"<b>%s</b> can be %s %s which means '%s.''\"%(head,' and '.join(is_mapp_),mapp_[part],eng+ch ))\n",
    "            else:\n",
    "                output.append(is_mapp_)\n",
    "                output.append(\"<b>%s</b> is %s %s which means '%s.'\"%(head,' and '.join(is_mapp_),mapp_[part],eng+ch))\n",
    "        else:\n",
    "            if 'A' in is_mapp_:\n",
    "                output.append(\"<b>%s</b> can be %s which means '%s.'\"%(head,mapp_[part],eng+ch))\n",
    "            else:\n",
    "                output.append(\"<b>%s</b> can be %s which means '%s.'\"%(head,mapp_[part],eng+ch))\n",
    "    return '<p>'+'</p><p>'.join(output)+'</p>'\n",
    "\n",
    "def explain_pattern(ex,head,part,pattern):\n",
    "#     [['N on how to inf%0', [], ['And , yes , I do send the parents home with instructions on how to use the drops if need be .', '', '']]]\n",
    "    output = []\n",
    "    if pattern == ex[0].split('%')[0]:\n",
    "        if pattern[-1].isupper():\n",
    "            output.append('The usage of <b>%s</b> is <b>%s</b>.'%(head,pattern))\n",
    "            output.append('For example:\\t%s'%('\\t'.join(ex[2][:-1])))\n",
    "        else:\n",
    "            output.append('The usage of <b>%s</b> is <b>%s</b>.'%(head,pattern.replace(part,head)))\n",
    "            output.append('For example:\\t%s'%('\\t'.join(ex[2][:-1])))\n",
    "    else:\n",
    "        if ex[0].split('%')[0].replace(part,head)[:3] != '(v)':\n",
    "            output.append('The usage of <b>%s</b> is <b>%s</b>.'%(head,'\" or \"'.join([ex[0].split('%')[0].replace(part,head),pattern.replace(part,head)])))\n",
    "        else:\n",
    "            output.append('The usage of <b>%s</b> is <b>%s</b>.'%(head,ex[0].split('%')[0].replace(part,head)))\n",
    "        output.append('For example:\\t%s'%('\\t'.join(ex[2][:-1])))\n",
    "    if miniparCol[head][ex[0].split('%')[0]]:\n",
    "        _pos = {'ADJ':'adjective','V':'verb','N':'noun'}\n",
    "        output.append(\"Besides, it is often paired the %s <b>%s</b> with vocabularies such as <b>%s</b>.\"%(_pos[part],head,','.join([v[0] for v in miniparCol[head][ex[0].split('%')[0]].most_common(3)])))\n",
    "    return '<p>'+'</p><p>'.join(output)+'</p>'\n",
    "\n",
    "def select_examples(pattern,head,part):\n",
    "    res = [item for item in dictWord[part][head] if pattern == item[0].split('%')[0]]\n",
    "    if res:\n",
    "        return res\n",
    "    res = [item for item in dictWord[part][head] if pattern== item[0].split('%')[0].replace('(v)','').strip() or pattern.replace('-ing','n')== item[0].split('%')[0].replace('(v)','').strip() or pattern.replace('pron-refl','n') == item[0].split('%')[0]]\n",
    "    return res\n",
    "\n",
    "def find_nextword(entails_sent,target,pos = ''):\n",
    "    idx = entails_sent.index(target)\n",
    "    default_idx = idx+1\n",
    "    default = entails_sent[default_idx]\n",
    "    tagging = geniatag(' '.join(entails_sent))\n",
    "    if not pos:\n",
    "        return default,tagging[1][default_idx][0]\n",
    "    else:\n",
    "        while(idx+1 < len(entails_sent)):\n",
    "            if tagging[2][idx][0][0] == pos:\n",
    "                return entails_sent[idx],tagging[1][idx][0]\n",
    "            else:\n",
    "                idx += 1\n",
    "        return default,tagging[1][default_idx][0]\n",
    "    \n",
    "def find_idx(tagging,target):\n",
    "    words = [key[0] for key in tagging[0]]\n",
    "    lemmas = [key[0] for key in tagging[1]]\n",
    "    tags = [key[0] for key in tagging[2]]\n",
    "    if target in words:\n",
    "        return words.index(target),words,lemmas,tags\n",
    "    else:\n",
    "        tmp = ' '.join(words)\n",
    "        idx = tmp.find(target)\n",
    "        return len(tmp[:idx].strip().split()),words,lemmas,tags\n",
    "\n",
    "def find_idioms(entails_sent,target):\n",
    "    tagging = geniatag(' '.join(entails_sent))\n",
    "    done = False\n",
    "    if len(target) == 1:\n",
    "        idx,words,lemmas,tags = find_idx(tagging,target[0])\n",
    "        phrase = target[0]\n",
    "        while idx >0:\n",
    "            idx -= 1\n",
    "            if tags[idx][0] in ['V','N','J'] or lemmas[idx] in allreserved:\n",
    "                head = lemmas[idx]\n",
    "                phrase = head +' '+ phrase\n",
    "                if dictPhrase[phrase]:\n",
    "                    done = True\n",
    "                    break\n",
    "    else:\n",
    "        for t in target:\n",
    "            if not done:\n",
    "                idx,words,lemmas,tags = find_idx(tagging,target[0].split()[0])\n",
    "                phrase = t\n",
    "            while idx >0 and not done:\n",
    "                idx -= 1\n",
    "                if tags[idx][0] in ['V','N','J'] or lemmas[idx] in allreserved:\n",
    "                    head = lemmas[idx]\n",
    "                    phrase = head +' '+ phrase\n",
    "                    if dictPhrase[phrase]:\n",
    "                        done = True\n",
    "                        break\n",
    "    if done:\n",
    "        return phrase,head\n",
    "    else:\n",
    "        return '',''\n",
    "\n",
    "def find_idioms_from_gps(gps):\n",
    "    output = []\n",
    "    for  pattern , head , part , _,_ in gps[::-1]:\n",
    "        if phraseV[head]:\n",
    "            phrases = find_phrases([pattern],head,part)\n",
    "            if phrases:\n",
    "                for phrase in phrases:\n",
    "                    if phrase.split('%')[0] in dictPhrase:\n",
    "                        output.append('<b>%s</b> is a phrase which means <b>%s</b>.'%(phrase.split('%')[0],' '.join(list(dictPhrase[phrase.split('%')[0]].values())[0][0])))\n",
    "                    if  '  '.join(phraseV[head][phrase][0][2][:2]):\n",
    "                        output.append(\"For example: %s\"%('  '.join(phraseV[head][phrase][0][2][:2])))\n",
    "    return '<p>'+'</p><p>'.join(output)+'</p>'\n",
    "                        \n",
    "def compare_lemma(correction):\n",
    "    dels = deletion.search(correction)\n",
    "    adds = addition.search(correction)\n",
    "    \n",
    "    d_tar = dels.group(1)\n",
    "    a_tar = adds.group(1)\n",
    "    \n",
    "    before = ' '.join(correction.replace(dels.group(0),dels.group(1)).replace(adds.group(0),'').split())\n",
    "    after = ' '.join(correction.replace(adds.group(0),adds.group(1)).replace(dels.group(0),'').split())\n",
    "    \n",
    "    before = geniatag(before)\n",
    "    after = geniatag(after)\n",
    "    idx,words,lemmas,tags = find_idx(before,d_tar)\n",
    "    if tags[idx][0] == 'J':\n",
    "        d = words[idx],lemmas[idx],'ADJ'\n",
    "    else:\n",
    "        d = words[idx],lemmas[idx],tags[idx][0]\n",
    "    idx,words,lemmas,tags = find_idx(after,a_tar)\n",
    "    if tags[idx][0] == 'J':\n",
    "        a = words[idx],lemmas[idx],'ADJ'\n",
    "    else:\n",
    "        a = words[idx],lemmas[idx],tags[idx][0]\n",
    "    return d,a\n",
    "\n",
    "def find_collocations(gps,tagging,a_lemma,d_lemma):\n",
    "    output = []\n",
    "    for gp in gps[::-1]:\n",
    "        pattern = ' '.join([g for g in gp[0].replace('(v)','').split() if g.strip()])\n",
    "        if len(pattern.split()) == 3 or pattern == 'V n':\n",
    "            headword, pos, sent = gp[1:]\n",
    "            idx,words,lemmas,tags = find_idx(tagging,sent.split()[-1])\n",
    "            tail = lemmas[idx]\n",
    "            if headword == a_lemma:\n",
    "                if miniparCol[a_lemma][pattern][lemmas[idx]] and miniparCol[d_lemma][pattern][lemmas[idx]]:\n",
    "                    if pattern == 'V n':\n",
    "                        col = ' '.join([a_lemma,tail])\n",
    "                        wcol = ' '.join([d_lemma,tail])\n",
    "                    else:\n",
    "                        prep = pattern.split()[1] \n",
    "                        col = ' '.join([a_lemma,prep,tail])\n",
    "                        wcol = ' '.join([d_lemma,prep,tail])\n",
    "                    percentage = miniparCol[a_lemma][pattern][tail] / (miniparCol[a_lemma][pattern][tail] + miniparCol[d_lemma][pattern][tail])\n",
    "                    output.append('<b>%s</b> is a more common collocation than <b>%s</b>.'%(col,wcol))\n",
    "                    output.append('The probability of using <b>%s</b> is %s%s.'%(col,\"{:3.2f}\".format(percentage*100),'%'))\n",
    "                elif miniparCol[headword][pattern][a_lemma]:\n",
    "                    if pattern == 'V n':\n",
    "                        col = ' '.join([headword,a_lemma])\n",
    "                        wcol = ' '.join([headword,d_lemma])\n",
    "                    else:\n",
    "                        prep = pattern.split()[1] \n",
    "                        col = ' '.join([headword,prep,a_lemma])\n",
    "                        wcol = ' '.join([headword,prep,d_lemma])\n",
    "                        output.append('People always use <b>%s</b>. It is impossible to use <b>%s</b>'%(col,wcol))\n",
    "            else:\n",
    "                if miniparCol[headword][pattern][a_lemma] and miniparCol[headword][pattern][d_lemma]:\n",
    "                    if pattern == 'V n':\n",
    "                        col = ' '.join([headword,a_lemma])\n",
    "                        wcol = ' '.join([headword,d_lemma])\n",
    "                    else:\n",
    "                        col = ' '.join([headword,pattern.split()[1],a_lemma])\n",
    "                        wcol = ' '.join([headword,pattern.split()[1],d_lemma])\n",
    "                    percentage = miniparCol[headword][pattern][a_lemma] / (miniparCol[headword][pattern][a_lemma] + miniparCol[headword][pattern][d_lemma])\n",
    "                    output.append('<b>%s</b> is a more common collocation than <b>%s</b>.'%(col,wcol))\n",
    "                    output.append('The probability of using <b>%s</b> is %s%s.'%(col,\"{:3.2f}\".format(percentage*100),'%'))\n",
    "                elif miniparCol[headword][pattern][a_lemma]:\n",
    "                    if pattern == 'V n':\n",
    "                        col = headword + a_lemma\n",
    "                    else:\n",
    "                        col = headword + ' ' +pattern.split()[1] + ' ' + a_lemma\n",
    "                        wcol = headword + ' ' +pattern.split()[1] + ' ' + d_lemma\n",
    "                        output.append('People always use <b>%s</b>. It is impossible to use <b>%s</b>'%(col,wcol))\n",
    "    return '<p>'+'</p><p>'.join(output)+'</p>'\n",
    "def explain_INF():\n",
    "    inf = \"When verbs followed by a to-infinitive often indicate the intention of an action or a future event.\"\n",
    "    ing = \"Compare: Verbs followed by an <b>-ing<b> form often emphasize on a status, fact, or activity.\"\n",
    "    return \"<p>\"+inf+\"</p><p>\"+ing+\"</p>\"\n",
    "def explain_INF():\n",
    "    ing = \"when verbs followed by an <b>-ing<b> form often emphasize on a status, fact, or activity.\"\n",
    "    inf = \"Compare: Verbs followed by a to-infinitive often indicate the intention of an action or a future event.\"\n",
    "    return \"<p>\"+ing+\"</p><p>\"+inf+\"</p>\"\n",
    "def check_P(word,lemma):\n",
    "    word = word.strip()\n",
    "    if word!= lemma:\n",
    "        if word[-1] == 's':\n",
    "            return True\n",
    "        return False\n",
    "    else:\n",
    "        if any([True for item in dictDef[word.strip()]['N'] if 'PLURAL' in item[0] or 'PLURAL' in re.findall(braces,item[2][0][0])]):\n",
    "            return True\n",
    "        elif any([True for item in dictDef[word.strip()]['N'] for i in re.findall(braces,item[2][0][0]) if 'plural' in i.lower()]):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "def check_U(word,lemma):\n",
    "    return any([True for item in dictDef[word.strip()]['N'] if 'U' in item[0] or 'U' in re.findall(braces,item[2][0][0])])\n",
    "def check_S(word,lemma):\n",
    "    if word == lemma:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "        \n",
    "def explain_time_error(head):\n",
    "    res = []\n",
    "#     in main part of the day\n",
    "    if head in ['morning','afternoon','evening']:\n",
    "        res.append(\"When you refer to 'main parts of the day' such as %s, use <b>in the</b> + time.\"%('morning/afternoon/evening'))\n",
    "        res.append(\"For example:\\tIn the %s we went for a walk along the Seine.\"%(head))\n",
    "        res.append(\"Compare:\\tat night: ‘I don’t like driving at night.’\")\n",
    "    elif head in MONTH or head.isdigit() and len(head)>2:\n",
    "        head = head[0].upper() + head[1:]\n",
    "        res.append(\"When you refer to 'months, years, centuries', use <b>in</b> + time.\")\n",
    "        if head in MONTH:\n",
    "            res.append(\"For example:\\tShe’ll be coming back home in %s.\"%(head))\n",
    "        else:\n",
    "            res.append(\"For example:\\tIn %s he decided to join the army.\"%(head))\n",
    "    elif head in SEASONS:\n",
    "        res.append(\"When you refer to 'seasons', use <b>in the</b> + time.\")\n",
    "        res.append(\"For example:\\tThey’re getting married in the %s\"%(head))\n",
    "    elif head in DATES:\n",
    "        head = head[0].upper() + head[1:]\n",
    "        res.append(\"When you refer to 'specific days/dates/ mornings/afternoons, etc', use 'on' + time.\")\n",
    "        re.append(\"For example:\\ton %s, on %s morning\"%(head,head))\n",
    "    elif head in HOLIDAY:\n",
    "        head = head[0].upper() + head[1:]\n",
    "        res.append(\"When you refer to 'the holiday period around Christmas, Easter, Hannukkah, Ramadan, etc', use 'at' + time.\")\n",
    "        res.append(\"For example:\\tWe like to stay at home at %s.\"%(head))\n",
    "        res.append(\"Compared:\\ton Christmas Day, on Easter Sunday\")\n",
    "    elif head in CLOCKTIME:\n",
    "        res.append(\"When you refer to <b>main points of time in the day (e.g: midnight/noon/dawn/lunch)</b>, use at + time.\")\n",
    "        res.append(\"For example:\\tWe usually open our presents at %s.\"%(heads))\n",
    "    elif head == 'weekend':\n",
    "        res.append(\"When you refer to 'weekend', use at the weekend. BUT (American English) on the weekend\")\n",
    "        res.append(\"For example:\\tWhat are you doing at the weekend?\")\n",
    "    elif head == 'weekends':\n",
    "        res.append(\"When you refer to <b>weekends</b>, use at the weekends.\")\n",
    "        res.append(\"For example:\\tI never do any work at weekends.\")\n",
    "    return '<p>'+'</p><p>'.join(res)+'</p>'\n",
    "\n",
    "def explain_replace(correction,entails_sent,correction_split,threshold,done = False):\n",
    "    output = []\n",
    "#     output.append('[ replace type ]')\n",
    "    if deletion.search(correction).group(1) in allreserved:\n",
    "        a_lemma = delandadd.search(correction).group(2)\n",
    "        # print('a_lemma',a_lemma)\n",
    "        nextword,nextword_lemma = find_nextword(entails_sent,a_lemma,'N')\n",
    "        nextdigit,nextdigit_lemma = find_nextword(entails_sent,a_lemma,'C')\n",
    "        if nextword.lower() in TIME or nextdigit.isdigit():\n",
    "            time_error = explain_time_error(nextword.lower())\n",
    "            if time_error:\n",
    "                output.append(time_error)\n",
    "                done = True\n",
    "        else:\n",
    "            target = [delandadd.search(correction).group(2)]\n",
    "            wedit = delandadd.search(correction).group(1)\n",
    "            gps = find_patterns(geniatag(' '.join(entails_sent)),target)\n",
    "            if gps:\n",
    "                gps = [(pattern,head,part,ex,1) if correction.find(ex) < threshold else (pattern,head,part,ex,-1) for pattern,head,part,ex in gps]\n",
    "                isTo = wedit=='to' or target[0]=='to'\n",
    "                if not any(p>0 and isTo for _,_,part,_,p in gps):\n",
    "                    if all( pattern.replace(target[0],wedit) in [key[0].split('%')[0].strip() for key in dictWord[part][head]] for pattern,head,part,_,p in gps):\n",
    "                        gps = gps[::-1]\n",
    "                        gps = sorted(gps,key = lambda x: pw_ratio[x[1]][(delandadd.search(correction).group(1),target[0])][x[4]],reverse = True)\n",
    "                    elif any( pattern.replace(target[0],wedit) in [key[0].split('%')[0].strip() for key in dictWord[part][head]] for pattern,head,part,_,p in gps):\n",
    "                        gps = [(pattern,head,part,ex,p) for pattern,head,part,ex,p in gps if pattern.replace(target[0],wedit) in [key[0].split('%')[0].strip() for key in dictWord[part][head]]]\n",
    "                    else:\n",
    "                        gps = gps[::-1]\n",
    "                        gps = sorted(gps,key = lambda x: pw_ratio[x[1]][(delandadd.search(correction).group(1),target[0])][x[4]],reverse = True)\n",
    "                else:\n",
    "                    headhalf = sorted([g for g in gps if g[4]>0],key = lambda x: pw_ratio[x[1]][(delandadd.search(correction).group(1),target[0])][x[4]],reverse = True)\n",
    "                    tailhalf = [g for g in gps[::-1] if g[4]<1]\n",
    "                    gps = headhalf + tailhalf\n",
    "                for  pattern , head , part , _,_ in gps:\n",
    "                    if head in dictWord[part]:\n",
    "                        examples = select_examples(pattern,head,part)\n",
    "                        if examples:\n",
    "                            for ex in examples:\n",
    "                                output.append(explain_pattern(ex,head,part,pattern))\n",
    "                            done = True\n",
    "                            break\n",
    "                    else:\n",
    "                            ini = ['V','N','ADJ']\n",
    "                            ini.remove(part)\n",
    "                            for pos in ini:\n",
    "                                examples = select_examples(pattern.replace(part,pos),head,pos)\n",
    "                                if examples:\n",
    "                                    for ex in examples:\n",
    "                                        output.append(explain_pattern(ex,head,pos,pattern.replace(part,pos)))\n",
    "                                        last_pat = pattern.split()[-1]\n",
    "                                        if part == 'V':\n",
    "                                            if isTo and last_pat == 'inf':\n",
    "                                                output.append(explain_INF())\n",
    "                                            elif isTo and last_pat == '-ing':\n",
    "                                                output.append(explain_ING())\n",
    "                                            else:\n",
    "                                                output.append(explain_VI_error(head,correction,pattern.replace(part,pos),ex))\n",
    "                                    done = True\n",
    "                                    break\n",
    "                if not done:\n",
    "                    output.append(find_idioms_from_gps(gps))\n",
    "            else:\n",
    "                idioms = find_idioms(entails_sent,target)\n",
    "                if idioms:\n",
    "                    output.append('\"<b>%s</b>\" is a phrase.'%(idioms[0]))\n",
    "                    output.append(\"This means that %s.\"%('\\t'.join(list(dictPhrase[idioms[0]].values())[0][0]).strip()))\n",
    "    else:\n",
    "        delset,addset = compare_lemma(correction)\n",
    "        d_word,d_lemma,d_part = delset\n",
    "        a_word,a_lemma,a_part = addset\n",
    "        d_lemma = d_lemma.lower()\n",
    "        a_lemma = a_lemma.lower()\n",
    "        parts = [d_part,a_part]\n",
    "        if d_lemma == a_lemma:\n",
    "            if 'V' in parts:\n",
    "                if d_lemma == a_lemma:\n",
    "                    target = [delandadd.search(correction).group(2)]\n",
    "                    gps = find_patterns(geniatag(' '.join(entails_sent)),target)\n",
    "                    if gps:\n",
    "                        for pattern,head,part,ex in gps:\n",
    "                            if ex.find(target[0]) > ex.find(head):\n",
    "                                if pattern.split()[-2] in allreserved:\n",
    "                                    output.append('The tense error is caused by <b>%s</b>.'%(pattern.split()[-2]))\n",
    "                                    break\n",
    "                                elif len(pattern.split()) and a_lemma != head:\n",
    "                                    output.append('The tense error is caused by <b>%s</b>.'%(head))\n",
    "                                    break\n",
    "                    else:\n",
    "                        output.append('Tense error!')\n",
    "                else:\n",
    "                    output.append(explain_voc_semantic_error(correction,d_lemma,d_part,a_lemma,a_part))\n",
    "            # countable uncountable?\n",
    "            elif 'N' in parts:\n",
    "#                 if a_word[-1] != 's' and d_word[-1]=='s' and d_lemma == a_lemma:\n",
    "                if check_U(a_word,a_lemma) and d_word[-1]=='s':\n",
    "#                     show_uncountable_meaning(addword)\n",
    "                    output.append('When you refer to an uncountable word, use the singular form.')\n",
    "                    output.append(find_N_meaning(a_lemma,'U'))\n",
    "                else:\n",
    "                    if d_lemma == a_lemma:\n",
    "                        output.append(find_N_meaning(a_lemma))\n",
    "                    else:\n",
    "                        output.append(explain_voc_semantic_error(correction,d_lemma,d_part,a_lemma,a_part))\n",
    "            else:\n",
    "                output.append(explain_voc_semantic_error(correction,d_lemma,d_part,a_lemma,a_part))\n",
    "        elif d_part != a_part:\n",
    "            output.append(\"Misuse part of speech!\")\n",
    "            output.append(\"%s: %s\\tvs\\t%s: %s\"%(d_word,d_part,a_word,a_part))\n",
    "        elif d_lemma in det_s.union(det_p) and a_lemma in det_s.union(det_p):\n",
    "#             an -> a\n",
    "            if d_lemma == 'an' and a_lemma == 'a':\n",
    "                nextword,nextword_lemma = find_nextword(entails_sent,a_lemma)\n",
    "                if nextword[0] == 'h':\n",
    "                    output.append(\"Before a word beginning with h, use a if the h is pronounced: <b>a house</b>, <b>a half</b>, <b>a horrible day</b>.\") \n",
    "                    output.append(\"Use an if the h is silent: <b>an hour</b>, <b>an honour</b>.\") \n",
    "                    output.append(\"If the h is pronounced but the syllable is unstressed, it is possible to use a or an (<b>a/an hotel</b>).\")  \n",
    "                    output.append(\"However, the use of an here is considered old fashioned and most people use a.\")\n",
    "                else:\n",
    "                    if nextword[0].lower() in ['u','o']:\n",
    "                        output.append(\"In this case, %s is pronounced as y which is a consonant sound, use a (NOT an).\"%(nextword[0].lower()))\n",
    "                    else:\n",
    "                        output.append(\"Always use a (NOT an) before a word beginning with a consonant sound.\")\n",
    "#             a -> an\n",
    "            elif d_lemma == 'a' and a_lemma == 'an':\n",
    "                nextword,nextword_lemma = find_nextword(entails_sent,a_lemma)\n",
    "                if nextword[0].lower() in vowel:\n",
    "                    if nextword[0] == 'h':\n",
    "                        output.append(\"Use an (NOT a) before words beginning with h when the h is not pronounced: <b>an honour</b> , <b>an hour</b>.\")\n",
    "                    else:\n",
    "                        output.append(\"Always use an (NOT a) before a word beginning with a vowel sound: <b>an %s</b> or <b>an %s</b>.\"%(vowelMap[nextword[0].lower()][0],vowelMap[nextword[0].lower()][1]))\n",
    "                elif nextword[0].isupper():\n",
    "                    output.append(\"Use an (NOT a) before an abbreviation that begins with a vowel sound\")\n",
    "            elif d_lemma == 'the' or a_lemma == 'the':\n",
    "                nextword,nextword_lemma = find_nextword(entails_sent,a_lemma,'N')\n",
    "                nextdigit,nextdigit_lemma = find_nextword(entails_sent,a_lemma,'C')\n",
    "                if nextword.lower() in TIME or nextdigit.isdigit():\n",
    "                    time_error = explain_time_error(nextword.lower())\n",
    "                    if time_error:\n",
    "                        output.append(time_error)\n",
    "                        done = True\n",
    "                else:\n",
    "                    output.append(dictDet[d_lemma])\n",
    "                    output.append(dictDet[a_lemma])\n",
    "            elif d_lemma=='any' and a_lemma in det_s.union(det_p):\n",
    "                if a_lemma in ['each','every','all'] : \n",
    "                    output.append('To refer to all the people or things in a group or category, use \"each/every + singular countable noun\" OR \"all + plural countable noun\".')\n",
    "                    output.append('[For example]:')\n",
    "                    output.append('‘Every house in the street had one or two broken windows.’')\n",
    "                    output.append('‘All students are required to register during the first week.’')\n",
    "                elif a_lemma in det_s:\n",
    "                    output.append(\"Any is usually used with uncountable nouns and plural countables (NOT with singular countable nouns).\")\n",
    "                    output.append(\"Compare: ‘Do you have any money?’ (money is an uncountable noun)\")\n",
    "                    output.append(\"‘Do you have any fifty-cent coins?’ (coins is a plural countable noun)\")\n",
    "                    output.append(\"‘Do you have a fifty-cent coin?’ (coin is a singular countable noun)\")\n",
    "                else:\n",
    "                    output.append(dictDet[d_lemma])\n",
    "                    output.append(dictDet[a_lemma])\n",
    "            elif d_lemma in det_p and a_lemma in det_s:\n",
    "                nextword,nextword_lemma = find_nextword(entails_sent,a_lemma,'N')\n",
    "                if check_S(nextword,nextword_lemma):\n",
    "                    output.append('<b>%s</b> is usually used with singular countable nouns.'%(a_lemma))\n",
    "                    output.append(find_N_meaning(nextword))\n",
    "                else:\n",
    "                    output.append(\"%s -> %s :  this case hasn't handled yet.\"%(d_lemma,a_lemma))\n",
    "            elif d_lemma in ['no','some'] and a_lemma == 'any':\n",
    "                output.append('After negative words, you usaully use \"any, anyone, anything, etc (Not some, someone, something, etc)\".')\n",
    "            elif 'at that moment' in ' '.join(entails_sent).lower():\n",
    "                output.append(\" When you are telling a story or reporting what happened, use <b>at that moment</b>\\t:\\tAt that moment the car skidded on the ice and went off the road.\")\n",
    "            elif d_lemma in det_s and a_lemma in det_p:\n",
    "                nextword,nextword_lemma = find_nextword(entails_sent,a_lemma,'N')\n",
    "                if check_S(nextword,nextword_lemma):\n",
    "                    output.append('<b>%s</b> is usually used with singular countable nouns. Besides, <b>%s</b> is usually used with uncountable nouns and plural countables (NOT with singular countable nouns)'%(d_lemma,a_lemma))\n",
    "                else:\n",
    "                    output.append('<b>%s</b> is usually used with <b>uncountable nouns</b> such as %s.'%(a_lemma,nextword))\n",
    "                    output.append(find_N_meaning(nextword,'U'))\n",
    "            elif d_lemma in det_p and a_lemma in det_s:\n",
    "                output.append('<b>%s</b> is usually used with plural countable nouns. Besides, <b>%s</b> is usually used with singular countable nouns.'%(d_lemma,a_lemma))\n",
    "            else:\n",
    "                output.append(dictDet[d_lemma])\n",
    "                output.append(dictDet[a_lemma])\n",
    "            \n",
    "        elif not parts[0].upper() in pos_map or not any([part for part in ['v','a','n'] if d_lemma in dictWord[pos_map[part.upper()]]]):\n",
    "            if tuple(sorted([d_lemma,a_lemma])) in aux:\n",
    "                output.append('Tense error!')\n",
    "            else:\n",
    "                output.append('It is a spelling error. To be more precisely, the spelling of <b>%s</b> is correct!'%(delandadd.search(correction).group(2)))\n",
    "        else:\n",
    "            output.append(explain_voc_semantic_error(correction,d_lemma,d_part,a_lemma,a_part))\n",
    "#             find collocation according to minipar\n",
    "            tagging = geniatag(' '.join(entails_sent))\n",
    "            gps = find_patterns(tagging,[a_word])\n",
    "            output.append(find_collocations(gps,tagging,a_lemma,d_lemma))\n",
    "    return output  \n",
    "\n",
    "def explain_unnecessary(correction,entails_sent,correction_split,threshold,done = False):\n",
    "    output = []\n",
    "#     output.append('[ Unnecessary type ]')\n",
    "    focus = deletion.search(correction).group(1)\n",
    "    focus = focus.lower()\n",
    "    idx = correction_split.index(deletion.search(correction).group(0))\n",
    "    target = []\n",
    "    if idx > 0:\n",
    "        target.append(correction_split[idx-1])\n",
    "    if idx+1 < len(correction_split):\n",
    "        target.append(correction_split[idx+1])\n",
    "    if focus in det_p.union(det_s) or focus.lower() == 'of':\n",
    "        if any(t in MONTH for t in target) and focus.lower() in ['the','of']: \n",
    "            output.append(\"When you say the date, use %s %s  or <br>WITHOUT the or of</br>.\"%(target[0],target[1],target[1],target[0]))\n",
    "        elif focus == 'the':\n",
    "            nextwordN,nextwordN_lemma = find_nextword(entails_sent,target[-1],'N')\n",
    "            done = True\n",
    "            if nextwordN[0].isupper():\n",
    "    #             Proprietary\n",
    "                output.append('Do not use <br>the</br> before the names of %s.'%(\"a language, disease, mountain, airports,railway stations, streets and roads\"))\n",
    "            elif check_P(nextwordN,nextwordN_lemma):\n",
    "    #             plural form of countable noun\n",
    "                output.append(\"Do not use the with the plural form of a countable noun when it is used in a general sense.\")\n",
    "                output.append(\"Compare: ‘She likes cats.’ (= cats in general)\")\n",
    "                output.append(\"The cats we saw in Venice looked very hungry.’ (= a particular group of cats)\")\n",
    "            elif check_uncountable(nextwordN):\n",
    "                output.append(\"Do not use the with an uncountable noun when it is used in a general sense:\")\n",
    "                output.append(\"‘She hates dishonesty.’\")\n",
    "                output.append(\"‘Power doesn’t interest him.’\")\n",
    "\n",
    "                output.append(\"The is used when the sense is restricted:\")\n",
    "                output.append(\"‘She hates the dishonesty of the man.’\") \n",
    "                output.append(\"‘The power enjoyed by politicians doesn’t interest him.’\")\n",
    "            else:\n",
    "                output.append(dictDet[focus])\n",
    "        elif any(t.lower() in TIME for t in target) and focus.lower() in det_s or focus in allreserved or focus.lower():\n",
    "            for t in target:\n",
    "                if t.lower() in TIME or t.isdigit():\n",
    "                    time_error = explain_time_error(t.lower())\n",
    "                    if time_error:\n",
    "                        output.append(time_error)\n",
    "                        done = True\n",
    "                        break\n",
    "        elif focus in det_s:\n",
    "            nextwordN,nextwordN_lemma = find_nextword(entails_sent,target[-1],'N')\n",
    "            nextwordJ,nextwordJ_lemma = find_nextword(entails_sent,target[-1],'J')\n",
    "    #             uncountable\n",
    "            if check_uncountable(nextwordN):\n",
    "                output.append('<b>%s</b> is uncountable so it must not be used with <b>%s</b> whcih is usually paired with singular countable nouns.'%(nextwordN,focus))\n",
    "                done = True\n",
    "            elif check_P(nextwordN,nextwordN_lemma):\n",
    "                output.append('<b>%s</b> is usually used with singular countable nouns.'%(focus))\n",
    "#             elif entails_sent.index(nextwordN) - entails_sent.index(nextwordJ) < 2:\n",
    "#     #             adjective to be noun\n",
    "#                 output.append(\"Do not use %s before an adjective (e.g. ‘deaf’, ‘British’) unless the adjective is followed by a noun\"%(focus))\n",
    "#                 output.append(dictDet[focus])\n",
    "#                 done = True\n",
    "        elif focus in det_p:\n",
    "            nextwordN,nextwordN_lemma = find_nextword(entails_sent,target[-1],'J')\n",
    "            if nextwordN == 'certain':\n",
    "                output.append(\"Do not use a determiner (e.g. some, the, their) before certain when it means ‘particular’.\")\n",
    "                output.append(dictDet[focus])\n",
    "                done = True\n",
    "        elif focus != 'that':\n",
    "            output.append(dictDet[focus])\n",
    "            done = True\n",
    "    if not done:\n",
    "        gps = find_patterns(geniatag(' '.join(entails_sent)),target)\n",
    "        if gps:\n",
    "            gps = [(pattern,head,part,ex,1) if correction.find(ex) < threshold else (pattern,head,part,ex,-1) for pattern,head,part,ex in gps]\n",
    "            isTo = focus=='to'\n",
    "            if not any(p>0 and isTo  for _,_,part,_,p in gps):\n",
    "                gps = gps[::-1]\n",
    "                gps = sorted(gps,key = lambda x: pw_ratio[x[1]][(focus,'')][x[4]],reverse = True)\n",
    "            else:\n",
    "                headhalf = sorted([g for g in gps if g[4]>0],key = lambda x: pw_ratio[x[1]][(focus,'')][x[4]],reverse = True)\n",
    "                tailhalf = [g for g in gps[::-1] if g[4]<1]\n",
    "                gps = headhalf + tailhalf\n",
    "            for pattern , head , part , _,_ in gps:\n",
    "                if head in  dictWord[part]:\n",
    "                    examples = select_examples(pattern,head,part)\n",
    "                    if examples:\n",
    "                        for ex in examples:\n",
    "                            last_pat = pattern.split()[-1]\n",
    "                            if part == 'V':\n",
    "                                if isTo and last_pat == '-ing':\n",
    "                                    output.append(explain_ING())\n",
    "                                    output.append(explain_pattern(ex,head,part,pattern))\n",
    "                                elif focus in allreserved:\n",
    "                                    output.append(explain_VT_error(head,correction,pattern,ex))\n",
    "                                else:\n",
    "                                    output.append(explain_pattern(ex,head,part,pattern))\n",
    "                            else:\n",
    "                                output.append(explain_pattern(ex,head,part,pattern))\n",
    "                        done = True\n",
    "                        break\n",
    "                else:\n",
    "                    ini = ['V','N','ADJ']\n",
    "                    ini.remove(part)\n",
    "                    for pos in ini:\n",
    "                        examples = select_examples(pattern.replace(part,pos),head,pos)\n",
    "                        if examples:\n",
    "                            for ex in examples:\n",
    "                                output.append(explain_pattern(ex,head,pos,pattern.replace(part,pos)))\n",
    "                                last_pat = pattern.split()[-1]\n",
    "                                if part == 'V' and last_pat != 'inf' and last_pat != '-ing':\n",
    "                                    output.append(explain_VI_error(head,correction,pattern.replace(part,pos),ex))\n",
    "                            done = True\n",
    "                            break\n",
    "                        if done:\n",
    "                            break\n",
    "            if not done:\n",
    "                output.append(find_idioms_from_gps(gps))\n",
    "        else:\n",
    "            \n",
    "            output.append('grammar error : missing prepositions or conjunctions')\n",
    "    return output\n",
    "            \n",
    "def explain_missing(correction,entails_sent,correction_split,threshold,done=False):\n",
    "    output = []\n",
    "#     output.append('[ Missing type]')\n",
    "    focus = addition.search(correction).group(1)\n",
    "    nextwordN,nextwordN_lemma = find_nextword(entails_sent,focus,'N')\n",
    "    nextwordV,nextwordV_lemma = find_nextword(entails_sent,focus,'V')\n",
    "    nextdigit,nextdigit_lemma = find_nextword(entails_sent,focus,'C')\n",
    "    if focus.lower() in det_s.union(det_s):\n",
    "        if focus in det_p:\n",
    "            output.append(\"Use a determiner before a plural countable noun such as %s\"%(nextwordN))\n",
    "            output.append(dictDet[focus.lower()])\n",
    "            output.append(find_N_meaning(nextwordN))\n",
    "            done = True\n",
    "        elif focus in det_s:\n",
    "            if focus != 'that':\n",
    "                output.append(\"Use a determiner before a singular countable noun such as %s\"%(nextwordN))\n",
    "                output.append(find_N_meaning(nextwordN))\n",
    "                done = True\n",
    "            else:\n",
    "                if  nextwordV!= nextwordN and entails_sent.index(nextwordV) - entails_sent.index(nextwordN)>2:\n",
    "                    output.append(\"Use a determiner before a singular countable noun such as %s\"%(nextwordN))\n",
    "                    output.append(find_N_meaning(nextwordN))\n",
    "                    done = True\n",
    "                else:\n",
    "                    output.append(\"Do not omit <br>that</br> before a clause.\")\n",
    "                    done = True\n",
    "        elif nextwordN.lower() in TIME or nextdigit.isdigit():\n",
    "            if focus.lower() =='the' or focus.lower() in det_s or focus.lower() in allreserved:\n",
    "                time_error = explain_time_error(nextwordN.lower())\n",
    "                if time_error:\n",
    "                    output.append(time_error)\n",
    "                    done = True\n",
    "        elif focus.lower() == 'the':\n",
    "            nextwordJ,nextwordJ_lemma = find_nextword(entails_sent,focus,'J')\n",
    "            if entails_sent.index(nextwordJ) >= entails_sent.index(nextwordN):\n",
    "    #             only N after the\n",
    "                if nextwordN[0].isupper():\n",
    "                    output.append(dictDet[focus.lower()])\n",
    "                    output.append(\"Always use <br>the</br> when you know that the person you are talking or writing to will understand which person, thing, or group you are referring to.\")\n",
    "                    output.append(\"Always use <br>the</br> with the name of canals (e.g. the Suez Canal),\")\n",
    "                    output.append(\"rivers (e.g. the River Thames),\")\n",
    "                    output.append(\"oceans (e.g. the Atlantic Ocean),\")\n",
    "                    output.append(\"plural names (e.g. the Philippines),\")\n",
    "                    output.append(\"any country whose name includes ‘state’, ‘union’, ‘republic’, ‘kingdom’ etc (e.g. the U.K, the United Kingdom),\")\n",
    "                    output.append(\"hotels,\")\n",
    "                    output.append(\"and restaurants (Note that names with a possessive form are exceptions: Tiffany's)\")\n",
    "                    done = True\n",
    "            else:\n",
    "    #             J after the\n",
    "                output.append(\"To refer to a group of people or stuff, use 'the + adjective'\")\n",
    "                output.append(\"[people]\")\n",
    "                output.append(\"the elderly\") \n",
    "                output.append(\"the British\")\n",
    "\n",
    "                output.append(\"[stuff]\")\n",
    "                output.append(\"the mysterious\")\n",
    "                output.append(\"the beautiful\")\n",
    "                done = True\n",
    "    if not done:\n",
    "        idx = [id for id,seg in enumerate(correction_split) if addition.search(correction).group(0) in seg][0]\n",
    "        target = []\n",
    "        if idx > 0:\n",
    "            target.append(' '.join([correction_split[idx-1],focus]))\n",
    "        if idx+1 < len(correction_split):\n",
    "            target.append(' '.join([focus,correction_split[idx+1]]))\n",
    "        gps = find_patterns(geniatag(' '.join(entails_sent)),target)\n",
    "        if focus in allreserved:\n",
    "            if gps:\n",
    "                gps = [(pattern,head,part,ex,1) if correction.find(ex) < threshold else (pattern,head,part,ex,-1) for pattern,head,part,ex in gps]\n",
    "                isTo = target[0]=='to'\n",
    "                if not any(p>0 and isTo for _,_,part,_,p in gps):\n",
    "                    gps = gps[::-1]\n",
    "                    gps = sorted(gps,key = lambda x: pw_ratio[x[1]][('',focus)][x[4]],reverse = True)\n",
    "                for pattern , head , part , _,_ in gps:\n",
    "                    if head in  dictWord[part]:\n",
    "                        if dictWord[part][head]: \n",
    "                            examples = select_examples(pattern,head,part)\n",
    "                            if examples:\n",
    "                                for ex in examples:\n",
    "                                    output.append(explain_pattern(ex,head,part,pattern))\n",
    "                                    last_pat = pattern.split()[-1]\n",
    "                                    if part == 'V':\n",
    "                                        if isTo and last_pat == 'inf':\n",
    "                                            output.append(explain_INF())\n",
    "                                        else:\n",
    "                                            output.append(explain_VI_error(head,correction,pattern,ex))\n",
    "                                done = True\n",
    "                                break\n",
    "                        else:\n",
    "                            ini = ['V','N','ADJ']\n",
    "                            ini.remove(part)\n",
    "                            for pos in ini:\n",
    "                                examples = select_examples(pattern.replace(part,pos),head,pos)\n",
    "                                if examples:\n",
    "                                    for ex in examples:\n",
    "                                        output.append(explain_pattern(ex,head,pos,pattern.replace(part,pos)))\n",
    "                                        last_pat = pattern.split()[-1]\n",
    "                                        if part == 'V' and last_pat != 'inf' and last_pat != '-ing':\n",
    "                                            output.append(explain_VI_error(head,correction,pattern.replace(part,pos),ex))\n",
    "                                    done = True\n",
    "                                    break\n",
    "                    else:\n",
    "                        output.append(find_voc_meaning(head,part))\n",
    "                        done = True\n",
    "                        break\n",
    "                if not done:\n",
    "                    output.append(find_idioms_from_gps(gps))\n",
    "            else:\n",
    "                phrase,head = find_idioms(entails_sent,target)\n",
    "                if phrase and head:\n",
    "                    done = True\n",
    "                    output.append('<b>%s</b> is a phrase. Its definition is listed in the below. <ol><li>%s</li></ol>'%(phrase,'</li><li>'.join(['\\t'.join(d).strip() for d in list(dictPhrase[phrase].values())[0][:2]])))\n",
    "                    for p in phraseV[head].keys():\n",
    "                        if phrase in p:\n",
    "                            output.append('For example: %s'%('  '.join([' '.join(phraseV[head][p][0][2][:2]) for p in phraseV[head].keys() if phrase in p ])))               \n",
    "        else:\n",
    "            if gps:\n",
    "                gps = [(pattern,head,part,ex,1) if correction.find(ex) < threshold else (pattern,head,part,ex,-1) for pattern,head,part,ex in gps]\n",
    "                gps = sorted(gps,key = lambda x: pw_ratio[x[1]][('',focus)][x[4]],reverse = True)\n",
    "                for pattern , head , part , _,_ in gps:\n",
    "                    if head in  dictWord[part]:\n",
    "                        examples = select_examples(pattern,head,part)\n",
    "                        if examples:\n",
    "                            for ex in examples:\n",
    "                                output.append(explain_pattern(ex,head,part,pattern))\n",
    "                            done = True\n",
    "                            break\n",
    "                    else:\n",
    "                            ini = ['V','N','ADJ']\n",
    "                            ini.remove(part)\n",
    "                            for pos in ini:\n",
    "                                examples = select_examples(pattern.replace(part,pos),head,pos)\n",
    "                                if examples:\n",
    "                                    for ex in examples:\n",
    "                                        output.append(explain_pattern(ex,head,pos,pattern.replace(part,pos)))\n",
    "                                        last_pat = pattern.split()[-1]\n",
    "                                        if part == 'V' and last_pat != 'inf' and last_pat != '-ing':\n",
    "                                            output.append(explain_VI_error(head,correction,pattern.replace(part,pos),ex))\n",
    "                                    done = True\n",
    "                                    break\n",
    "                if not done:\n",
    "                    output.append(find_idioms_from_gps(gps))\n",
    "            else:\n",
    "                delword,part_ = my_lemma(correction,entails_sent)\n",
    "                if part_ in ['N','J','V']:\n",
    "                    if part_ == 'J':\n",
    "                        part = 'ADJ'\n",
    "                    output.append(find_voc_meaning(delword,part_))\n",
    "                else:\n",
    "                    phrase,head = find_idioms(entails_sent,target)\n",
    "                    if phrase and head:\n",
    "                        done = True\n",
    "                        output.append('<b>%s</b> is a phrase. Its definition is listed in the below. <ol><li>%s</li></ol>'%(phrase,'</li><li>'.join([' '.join(d) for d in list(dictPhrase[phrase].values())[0][:2]])))\n",
    "                        output.append('For example: %s'%('  '.join([' '.join(phraseV[head][p][0][2][:2]) for p in phraseV[head].keys() if phrase in p ])))\n",
    "    return output\n",
    "\n",
    "def leave_error(correction,lists):\n",
    "    head = lists[0]\n",
    "    tail = lists[1]\n",
    "    input_cor = \"\"\n",
    "    input_split = []\n",
    "    if head or tail:\n",
    "        if head and tail:\n",
    "            start = correction.find(head)\n",
    "            idx = start + len(head)\n",
    "        elif head:\n",
    "            start = correction.find(head)\n",
    "            idx = start + len(head)\n",
    "        elif tail:\n",
    "            start = correction.find(tail)\n",
    "            idx = start + len(tail)\n",
    "        if idx < len(correction):\n",
    "            input_cor = correction[:idx+1]+ ' '.join(rephrase(correction[idx:]))\n",
    "            input_split = input_cor.split()\n",
    "        return input_cor,input_split,start\n",
    "    \n",
    "def explain(corrections):\n",
    "    result = dict()\n",
    "    mod_list = []\n",
    "    prevs = []\n",
    "    for correction in sent_tokenize(corrections):\n",
    "        correction = beautify(correction)\n",
    "        entails_sent = rephrase(correction)\n",
    "        while deletion.search(correction) or addition.search(correction):\n",
    "            if prevs:\n",
    "                if prevs[0] in allreserved or prevs[1] in allreserved:\n",
    "                    if deletion.search(correction) and addition.search(correction):\n",
    "                        a = deletion.search(correction).group(0)\n",
    "                        b = addition.search(correction).group(0)\n",
    "                        c = addition.search(correction).group(1)\n",
    "                        if 'ing' in a or 'ing' in b:\n",
    "                            correction = ' '.join(correction.replace(a,'').replace(b,c).split())     \n",
    "                            continue\n",
    "            done = False\n",
    "            \n",
    "            case1 = 100000\n",
    "            case2 = 100000\n",
    "            case3 = 100000\n",
    "            replacelist = [[],[],[]]\n",
    "            if delandadd.search(correction):\n",
    "                tmp = multi_delandadd.search(correction).group(0).strip()\n",
    "                # before = ''.join(tmp.split())\n",
    "                before = ' '.join(re.findall(addition, tmp))\n",
    "#                 transform = ' '.join(re.findall(deletion, tmp)) + '\\t->\\t' + before\n",
    "                transform = 'Replace ' + ' '.join(re.findall(deletion, tmp)) + ' with ' + before\n",
    "                after = before\n",
    "                case1 = correction.find(tmp)\n",
    "                replacelist[0] = [tmp,after,transform] \n",
    "                prevs = ( ' '.join(re.findall(deletion, tmp)),before)\n",
    "            if deletion.search(correction):\n",
    "                tmp = deletion.search(correction).group(0)\n",
    "                after = ''\n",
    "                case2 = correction.find(tmp)\n",
    "#                 transform = deletion.search(correction).group(1) + '\\t->\\t' + 'NONE'\n",
    "                transform = 'Omit ' + deletion.search(correction).group(1) \n",
    "                replacelist[1] = [tmp,after,transform] \n",
    "                prevs = ( deletion.search(correction).group(1)  ,'')\n",
    "            if addition.search(correction):\n",
    "                tmp = addition.search(correction).group(0)\n",
    "                after = addition.search(correction).group(1)\n",
    "                case3 = correction.find(tmp)\n",
    "#                 transform = 'NONE' + '\\t->\\t' + addition.search(correction).group(1)\n",
    "                transform = 'Insert ' + addition.search(correction).group(1)\n",
    "                replacelist[2] = [tmp,after,transform] \n",
    "                prev = ('',addition.search(correction).group(1))\n",
    "            idx = min([case1, case2,case3])\n",
    "            idx = [case1, case2,case3].index(idx)\n",
    "            \n",
    "            input_cor,input_split,threshold = leave_error(correction,replacelist[idx])\n",
    "            # replace\n",
    "            if idx == 0:\n",
    "                tmp = explain_replace(input_cor,entails_sent,input_split,threshold,done)\n",
    "            # deletion\n",
    "            elif idx == 1:\n",
    "                tmp = explain_unnecessary(input_cor,entails_sent,input_split,threshold,done)\n",
    "            # addition\n",
    "            elif idx == 2 :\n",
    "                tmp = explain_missing(input_cor,entails_sent,input_split,threshold,done)\n",
    "            result[replacelist[idx][2]] = tmp  \n",
    "            correction = ' '.join(correction.replace(replacelist[idx][0],replacelist[idx][1]).split())\n",
    "            \n",
    "            mod_list.append(replacelist[idx][2])\n",
    "    return result,mod_list\n",
    "\n",
    "def beautify(s):\n",
    "    for a in re.findall(ab,s):\n",
    "        if a.lower() in abbrs:\n",
    "            s = s.replace(a,abbrs[a.lower()])\n",
    "    tokens = [ss for ss in s.split() if ss.strip()]\n",
    "    s = ' '.join(tokens)\n",
    "    s_tmp = s\n",
    "    while loss_add.search(s_tmp) or loss_del.search(s_tmp):\n",
    "        if loss_del.search(s_tmp):\n",
    "            head = loss_del.search(s_tmp).group(0)\n",
    "            tail = loss_del.search(s_tmp).group(1)\n",
    "            s = s.replace(head,'[-'+tail+'-]')\n",
    "            s_tmp = s_tmp.replace(head,tail)\n",
    "        if loss_add.search(s_tmp):\n",
    "            head = loss_add.search(s_tmp).group(0)\n",
    "            tail = loss_add.search(s_tmp).group(1)\n",
    "            s = s.replace(head,'{+'+tail+'+}')\n",
    "            s_tmp = s_tmp.replace(head,tail)\n",
    "    return ' '.join([ss.strip() for ss in s.split()])\n",
    "    \n",
    "def init_DB():\n",
    "    tmp_dictWord = eval(open('/home/nlplab/yeema/ErrorExplaination/GPs.linggle.extend.txt', 'r').read())\n",
    "    tmp_phrase = eval(open('/home/nlplab/yeema/ErrorExplaination/phrase.txt', 'r').read())\n",
    "    tmp_dictDef = eval(open('/home/nlplab/yeema/ErrorExplaination/cambridge.gps.semanticDict_word_v4.json').read())\n",
    "    tmp_dictPhrase = eval(open('/home/nlplab/yeema/ErrorExplaination/cambridge.gps.semanticDict_phrase_v5.json').read())\n",
    "    tmp_miniparCol = eval(open('/home/nlplab/yeema/grammarpat/minipar.collocation.json').read())\n",
    "    tmp_pw = open('/home/nlplab/yeema/problemWords/problem.col.v2').readlines()\n",
    "    for pos, values in tmp_dictWord.items():\n",
    "        for head,value in values.items():\n",
    "            dictWord[pos][head] = value\n",
    "            \n",
    "    for key,phrase in tmp_phrase.items():\n",
    "        for p,pat in phrase.items():\n",
    "            phraseV[key][p] = pat\n",
    "            \n",
    "    for head,values in tmp_dictPhrase.items():\n",
    "        for attr,value in values.items():\n",
    "            dictPhrase[head][attr] = value\n",
    "            \n",
    "    for head,values in tmp_dictDef.items():\n",
    "        for pos,value in values.items():\n",
    "            dictDef[head][pos] = value\n",
    "    \n",
    "    for head,values in tmp_miniparCol.items():\n",
    "        for pat,tails in values.items():\n",
    "            for tail, c in tails.items():\n",
    "                miniparCol[head][pat][tail] = int(c)\n",
    "    for line in tmp_pw:\n",
    "        line = line.strip()\n",
    "        line = line.split('\\t')\n",
    "        head, edit, p, c = line\n",
    "        edit = eval(edit)\n",
    "        p = int(p)\n",
    "        c = int(c)\n",
    "        pw[head][edit][p] = c\n",
    "    for head,edits in pw.items():\n",
    "        s = sum([j for i in pw[head].values() for j in i.values()])\n",
    "        for edit,count in edits.items():\n",
    "            for key,c in count.items():\n",
    "                if key >0:\n",
    "                    pw_ratio[head][edit][1] += c/s\n",
    "                elif key < 0:\n",
    "                    pw_ratio[head][edit][-1] += c/s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grep_error(string,lists,error,start):\n",
    "    for l in lists:\n",
    "        idx = string.find(l,start)\n",
    "        error.append((l,idx,idx+len(l)))\n",
    "        start += 1\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rephrase(sent):\n",
    "    words = []\n",
    "    for s in word_tokenize(sent):\n",
    "        if not s in set('[|]|{|}'.split('|')):\n",
    "            if s[0]== '+' and s[-1] =='+':\n",
    "                words.append(s[1:-1])\n",
    "            elif s[0]=='-' and s[-1] == '-':\n",
    "                continue\n",
    "            else:\n",
    "                words.append(s)\n",
    "    return words\n",
    "def explain(corrections):\n",
    "    result = dict()\n",
    "    mod_list = []\n",
    "    prevs = []\n",
    "    error_list = []\n",
    "    accumulate_len = 0\n",
    "    for correction in sent_tokenize(corrections):\n",
    "        correction = beautify(correction)\n",
    "        entails_sent = rephrase(correction)\n",
    "        print(correction,accumulate_len)\n",
    "        grep_error(correction,re.findall(r'\\[- *[^\\[\\]]* *-\\] *\\{\\+ *[^\\[\\]]* *\\+\\}|\\[- *[^\\[\\]]* *-\\]|\\{\\+ *[^\\[\\]]* *\\+\\}',correction),error_list,accumulate_len)\n",
    "        print(accumulate_len)\n",
    "        accumulate_len += len(correction)\n",
    "        print(error_list)\n",
    "        while deletion.search(correction) or addition.search(correction):\n",
    "            if prevs:\n",
    "                if prevs[0] in allreserved or prevs[1] in allreserved:\n",
    "                    if deletion.search(correction) and addition.search(correction):\n",
    "                        a = deletion.search(correction).group(0)\n",
    "                        b = addition.search(correction).group(0)\n",
    "                        c = addition.search(correction).group(1)\n",
    "                        if 'ing' in a or 'ing' in b:\n",
    "                            correction = ' '.join(correction.replace(a,'').replace(b,c).split())     \n",
    "                            continue\n",
    "            done = False\n",
    "            \n",
    "            case1 = 100000\n",
    "            case2 = 100000\n",
    "            case3 = 100000\n",
    "            replacelist = [[],[],[]]\n",
    "            if delandadd.search(correction):\n",
    "                tmp = multi_delandadd.search(correction).group(0).strip()\n",
    "                before = ' '.join(re.findall(addition, tmp))\n",
    "                transform = 'Replace ' + ' '.join(re.findall(deletion, tmp)) + ' with ' + before\n",
    "                after = before\n",
    "                case1 = correction.find(tmp)\n",
    "                replacelist[0] = [tmp,after,transform] \n",
    "                prevs = ( ' '.join(re.findall(deletion, tmp)),before)\n",
    "            if deletion.search(correction):\n",
    "                tmp = deletion.search(correction).group(0)\n",
    "                after = ''\n",
    "                case2 = correction.find(tmp)\n",
    "                transform = 'Omit ' + deletion.search(correction).group(1) \n",
    "                replacelist[1] = [tmp,after,transform] \n",
    "                prevs = ( deletion.search(correction).group(1)  ,'')\n",
    "            if addition.search(correction):\n",
    "                tmp = addition.search(correction).group(0)\n",
    "                after = addition.search(correction).group(1)\n",
    "                case3 = correction.find(tmp)\n",
    "#                 transform = 'NONE' + '\\t->\\t' + addition.search(correction).group(1)\n",
    "                transform = 'Insert ' + addition.search(correction).group(1)\n",
    "                replacelist[2] = [tmp,after,transform] \n",
    "                prev = ('',addition.search(correction).group(1))\n",
    "            idx = min([case1, case2,case3])\n",
    "            idx = [case1, case2,case3].index(idx)\n",
    "            \n",
    "            input_cor,input_split,threshold = leave_error(correction,replacelist[idx])\n",
    "            result[replacelist[idx][2]] = tmp  \n",
    "            correction = ' '.join(correction.replace(replacelist[idx][0],replacelist[idx][1]).split())\n",
    "            \n",
    "            mod_list.append(replacelist[idx][2])\n",
    "    return result,mod_list\n",
    "\n",
    "def beautify(s):\n",
    "    for a in re.findall(ab,s):\n",
    "        if a.lower() in abbrs:\n",
    "            s = s.replace(a,abbrs[a.lower()])\n",
    "    tokens = [ss for ss in s.split() if ss.strip()]\n",
    "    s = ' '.join(tokens)\n",
    "    s_tmp = s\n",
    "    while loss_add.search(s_tmp) or loss_del.search(s_tmp):\n",
    "        if loss_del.search(s_tmp):\n",
    "            head = loss_del.search(s_tmp).group(0)\n",
    "            tail = loss_del.search(s_tmp).group(1)\n",
    "            s = s.replace(head,'[-'+tail+'-]')\n",
    "            s_tmp = s_tmp.replace(head,tail)\n",
    "        if loss_add.search(s_tmp):\n",
    "            head = loss_add.search(s_tmp).group(0)\n",
    "            tail = loss_add.search(s_tmp).group(1)\n",
    "            s = s.replace(head,'{+'+tail+'+}')\n",
    "            s_tmp = s_tmp.replace(head,tail)\n",
    "    return ' '.join([ss.strip() for ss in s.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = \"[-The-] {+life+} [-in-] the {+countryside+} is quiet and relaxed.\"\n",
    "# result,mod_list =explain(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ab = re.compile(r\"\\w*'\\w*|\\w*’\\w*\")\n",
    "loss_del = re.compile(r'\\[ *- *([^\\[\\]]*?) *- *\\]')\n",
    "loss_add = re.compile(r'\\{\\ *\\+ *([^\\[\\]]*?) *\\+ *\\}')\n",
    "delandadd = re.compile(r'\\[- *([^\\[\\]]*?) *-\\] *\\{\\+ *([^\\[\\]]*?) *\\+\\}')\n",
    "deletion = re.compile(r'\\[- *([^\\[\\]]*?) *-\\]')\n",
    "addition = re.compile(r'\\{\\+ *([^\\[\\]]*?) *\\+\\}')\n",
    "braces = re.compile(r'\\[ *(.*?) *\\]')\n",
    "multi_delandadd = re.compile(r'(\\[-([^\\[\\]]*?)-\\] *\\{\\+([^\\{\\}]*?)\\+\\} *)+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'[-The-] {+life+} [-in-] the countryside is quiet and relaxed.'.find('[-in-]',10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Replace The with life': '{+life+}', 'Omit in': '[-in-]'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = \"He [-borrowed-]{+lent+} me some of his books.\"\n",
    "result,mod_list =explain(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Replace borrowed with lent': ['<p>It is a semantic error.</p><p><b>borrow</b> :\\t  to get or receive something from someone with the intention of giving it back after a period of time ( 借，借入 ).</p><p><b>lend</b> :\\t  to give something to someone for a short period of time, expecting it to be given back ( 借出，借給 ).</p>',\n",
       "  '<p></p>']}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = \"I shall never forget the day he died [-on-]{+in+} my arms.\"\n",
    "result,mod_list = explain(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Replace on with in': ['<p>The usage of <b>arm</b> is <b>in N</b>.</p><p>For example:\\tHe took/held her in his arms (= held her closely).\\t他把她摟進懷中。</p>']}"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = \"It’s also worth [-to-] [-visit-]{+visiting+} the north of England if you have time.\"\n",
    "result,mod_list = explain(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Omit to': [\"<p>The usage of <b>worth</b> is <b>worth -ing</b>.</p><p>For example:\\tThere's nothing worth reading in this newspaper.\\t這份報紙中沒甚麼值得一讀。</p>\"]}"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [(edits,count.items()) for edits,count in pw_ratio['die'].items() if edits[1] == 'in'],[(edits, count.values()) for edits,count in pw_ratio['arm'].items() if edits[1] == 'in']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('', 'at'), Counter({1: 0.16287015945330297})),\n",
       " (('in', 'at'), Counter({1: 0.1378132118451025})),\n",
       " (('to', 'at'), Counter({1: 0.13097949886104784})),\n",
       " (('', 'the'), Counter({1: 0.10136674259681093})),\n",
       " (('i', 'I'), Counter({-1: 0.09339407744874716})),\n",
       " (('', 'to'), Counter({-1: 0.06605922551252848})),\n",
       " (('when', 'When'), Counter({-1: 0.03530751708428246})),\n",
       " (('', 'I'), Counter({-1: 0.03302961275626424})),\n",
       " (('', 'will'), Counter({-1: 0.02847380410022779})),\n",
       " (('on', 'at'), Counter({1: 0.025056947608200455})),\n",
       " (('at', ''), Counter({1: 0.023917995444191344})),\n",
       " (('in', ''), Counter({1: 0.022779043280182234})),\n",
       " (('the', ''), Counter({1: 0.022779043280182234})),\n",
       " (('to', 'in'), Counter({1: 0.02164009111617312})),\n",
       " (('', 'in'), Counter({1: 0.02050113895216401})),\n",
       " (('', 'you'), Counter({-1: 0.018223234624145785})),\n",
       " (('in', 'to'), Counter({1: 0.017084282460136675})),\n",
       " (('to', ''), Counter({-1: 0.014806378132118452})),\n",
       " (('', 'a'), Counter({-1: 0.01366742596810934})),\n",
       " ((',', ''), Counter({-1: 0.010250569476082005}))]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted( pw_ratio['arrive'].items(), key = lambda x: sum(x[1].values()),reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('official', 72),\n",
       " ('President', 65),\n",
       " ('him', 52),\n",
       " ('leader', 36),\n",
       " ('them', 30)]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "miniparCol['discuss']['V with n'].most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('detail', 69), ('meeting', 26), ('talk', 15), ('public', 13), ('Moscow', 9)]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "miniparCol['discuss']['V in n'].most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
